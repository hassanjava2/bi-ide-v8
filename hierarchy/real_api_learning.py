"""
ğŸŒ REAL API LEARNING SYSTEM - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù…Ù† APIs
ÙƒÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© - Ù„Ø§ Ù…Ø­Ø§ÙƒØ§Ø©
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import threading
import time
import json
import requests
import os
from datetime import datetime
from pathlib import Path

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class RealAPIDataFetcher:
    """Ø¬Ø§Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† APIs Ù…ÙØªÙˆØ­Ø©"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'BI-IDE-Training-System/1.0'
        })
        self.cache = {}
        self.last_fetch = {}
    
    def fetch_github_trending(self):
        """Ø¬Ù„Ø¨ Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª GitHub Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© - Ø­Ù‚ÙŠÙ‚ÙŠ 100%"""
        try:
            # GitHub Search API - Ù…Ø¬Ø§Ù†ÙŠ
            url = "https://api.github.com/search/repositories"
            params = {
                "q": "language:python stars:>10000",
                "sort": "stars",
                "order": "desc",
                "per_page": 5
            }
            response = self.session.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                repos = [item["full_name"] for item in data.get("items", [])]
                return {"type": "code", "source": "GitHub API", "repos": repos, "timestamp": datetime.now().isoformat()}
        except Exception as e:
            print(f"    âš ï¸ GitHub API Error: {e}")
        return None
    
    def fetch_crypto_prices(self):
        """Ø¬Ù„Ø¨ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø¹Ù…Ù„Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© - Ø­Ù‚ÙŠÙ‚ÙŠ Ù…Ù† CoinGecko"""
        try:
            # CoinGecko API - Ù…Ø¬Ø§Ù†ÙŠ
            url = "https://api.coingecko.com/api/v3/simple/price"
            params = {
                "ids": "bitcoin,ethereum,cardano,solana",
                "vs_currencies": "usd",
                "include_24hr_change": "true"
            }
            response = self.session.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                markets = {}
                for coin, info in data.items():
                    markets[coin.upper()] = {
                        "price": info["usd"],
                        "change_24h": info.get("usd_24h_change", 0)
                    }
                return {"type": "markets", "source": "CoinGecko API", "markets": markets, "timestamp": datetime.now().isoformat()}
        except Exception as e:
            print(f"    âš ï¸ CoinGecko API Error: {e}")
        return None
    
    def fetch_hackernews(self):
        """Ø¬Ù„Ø¨ Ø£Ø®Ø¨Ø§Ø± ØªÙ‚Ù†ÙŠØ© Ù…Ù† HackerNews - Ø­Ù‚ÙŠÙ‚ÙŠ"""
        try:
            # HackerNews API - Ù…Ø¬Ø§Ù†ÙŠ
            top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = self.session.get(top_stories_url, timeout=10)
            if response.status_code == 200:
                story_ids = response.json()[:3]
                stories = []
                for story_id in story_ids:
                    story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    story_resp = self.session.get(story_url, timeout=10)
                    if story_resp.status_code == 200:
                        story = story_resp.json()
                        stories.append(story.get("title", "No title"))
                return {"type": "tech", "source": "HackerNews API", "stories": stories, "timestamp": datetime.now().isoformat()}
        except Exception as e:
            print(f"    âš ï¸ HackerNews API Error: {e}")
        return None
    
    def fetch_reddit_programming(self):
        """Ø¬Ù„Ø¨ Ù†Ù‚Ø§Ø´Ø§Øª Ø¨Ø±Ù…Ø¬ÙŠØ© Ù…Ù† Reddit - Ø­Ù‚ÙŠÙ‚ÙŠ"""
        try:
            # Reddit JSON API - Ø¹Ø§Ù…
            url = "https://www.reddit.com/r/programming/top.json"
            params = {"limit": 3, "t": "day"}
            headers = {"User-Agent": "BI-IDE-Training/1.0"}
            response = self.session.get(url, params=params, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                posts = [post["data"]["title"] for post in data["data"]["children"]]
                return {"type": "discussion", "source": "Reddit API", "posts": posts, "timestamp": datetime.now().isoformat()}
        except Exception as e:
            print(f"    âš ï¸ Reddit API Error: {e}")
        return None
    
    def fetch_stackoverflow_questions(self):
        """Ø¬Ù„Ø¨ Ø£Ø³Ø¦Ù„Ø© StackOverflow - Ø­Ù‚ÙŠÙ‚ÙŠ"""
        try:
            # StackExchange API - Ù…Ø¬Ø§Ù†ÙŠ
            url = "https://api.stackexchange.com/2.3/questions"
            params = {
                "order": "desc",
                "sort": "votes",
                "tagged": "python;pytorch",
                "site": "stackoverflow",
                "pagesize": 3
            }
            response = self.session.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                questions = [item["title"] for item in data.get("items", [])]
                return {"type": "qa", "source": "StackOverflow API", "questions": questions, "timestamp": datetime.now().isoformat()}
        except Exception as e:
            print(f"    âš ï¸ StackOverflow API Error: {e}")
        return None
    
    def fetch_cve_security(self):
        """Ø¬Ù„Ø¨ Ø«ØºØ±Ø§Øª Ø£Ù…Ù†ÙŠØ© Ù…Ù† CVE - Ø­Ù‚ÙŠÙ‚ÙŠ"""
        try:
            # CVE API - Ø¹Ø§Ù…
            url = "https://cve.circl.lu/api/last"
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                cves = []
                for item in data[:3]:
                    cves.append(f"{item['id']}: {item.get('summary', 'No summary')[:100]}")
                return {"type": "security", "source": "CVE API", "cves": cves, "timestamp": datetime.now().isoformat()}
        except Exception as e:
            print(f"    âš ï¸ CVE API Error: {e}")
        return None
    
    def fetch_arxiv_papers(self):
        """Ø¬Ù„Ø¨ Ø£Ø¨Ø­Ø§Ø« Ù…Ù† arXiv - Ø­Ù‚ÙŠÙ‚ÙŠ"""
        try:
            # arXiv API - Ù…Ø¬Ø§Ù†ÙŠ
            url = "http://export.arxiv.org/api/query"
            params = {
                "search_query": "cat:cs.AI",
                "start": 0,
                "max_results": 3,
                "sortBy": "submittedDate",
                "sortOrder": "descending"
            }
            response = self.session.get(url, params=params, timeout=10)
            if response.status_code == 200:
                import xml.etree.ElementTree as ET
                root = ET.fromstring(response.content)
                papers = []
                ns = {'atom': 'http://www.w3.org/2005/Atom'}
                for entry in root.findall('atom:entry', ns):
                    title = entry.find('atom:title', ns)
                    if title is not None:
                        papers.append(title.text.strip().replace('\n', ' '))
                return {"type": "research", "source": "arXiv API", "papers": papers, "timestamp": datetime.now().isoformat()}
        except Exception as e:
            print(f"    âš ï¸ arXiv API Error: {e}")
        return None
    
    def fetch_for_layer(self, layer_name):
        """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø­Ø³Ø¨ Ø§Ø®ØªØµØ§Øµ Ø§Ù„Ø·Ø¨Ù‚Ø©"""
        # Rate limiting - Ù„Ø§ Ù†Ø¬Ù„Ø¨ ÙƒÙ„ Ù…Ø±Ø©
        current_time = time.time()
        if layer_name in self.last_fetch:
            if current_time - self.last_fetch[layer_name] < 60:  # Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø©
                return self.cache.get(layer_name)
        
        result = None
        source = "Unknown"
        
        if "code" in layer_name or "builder" in layer_name or "ide" in layer_name:
            result = self.fetch_github_trending() or self.fetch_stackoverflow_questions()
            source = "GitHub/StackOverflow"
        
        elif "erp" in layer_name or "business" in layer_name or "council" in layer_name or "president" in layer_name:
            result = self.fetch_crypto_prices() or self.fetch_hackernews()
            source = "CoinGecko/HackerNews"
        
        elif "guardian" in layer_name or "security" in layer_name:
            result = self.fetch_cve_security()
            source = "CVE Database"
        
        elif "scouts" in layer_name or "research" in layer_name or "experts" in layer_name or "learning" in layer_name:
            result = self.fetch_arxiv_papers()
            source = "arXiv"
        
        elif "meta" in layer_name or "execution" in layer_name:
            result = self.fetch_reddit_programming() or self.fetch_hackernews()
            source = "Reddit/HackerNews"
        
        else:
            result = self.fetch_hackernews()
            source = "HackerNews"
        
        # ØªØ®Ø²ÙŠÙ† ÙÙŠ cache
        if result:
            self.cache[layer_name] = result
            self.last_fetch[layer_name] = current_time
            print(f"    âœ… Fetched REAL data from {source}")
        
        return result

class RealDataset(Dataset):
    """Dataset ÙŠØ³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† APIs"""
    
    def __init__(self, layer_name, fetcher):
        self.layer_name = layer_name
        self.fetcher = fetcher
        self.seq_length = 128
        self.data = []
        self.vocab = {}
        self.fetch_count = 0
        self.refresh()
    
    def tokenize(self, text):
        """ØªØ­ÙˆÙŠÙ„ Ù†Øµ Ù„Ù€ tokens"""
        tokens = []
        for word in str(text).lower().split():
            if word not in self.vocab:
                self.vocab[word] = len(self.vocab) % 10000
            tokens.append(self.vocab[word])
        # padding
        tokens += [0] * (self.seq_length - len(tokens))
        return tokens[:self.seq_length]
    
    def refresh(self):
        """Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© - ÙˆØªØ­Ø±ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        print(f"ğŸŒ [{self.layer_name}] Fetching REAL data from Internet APIs...")
        
        data = self.fetcher.fetch_for_layer(self.layer_name)
        samples = []
        
        if data:
            self.fetch_count += 1
            
            if data.get("type") == "code":
                for repo in data.get("repos", [])[:2]:
                    text = f"Repository {repo} is trending with high quality code for machine learning"
                    samples.append(self.tokenize(text))
                for qa in data.get("questions", []):
                    samples.append(self.tokenize(qa))
            
            elif data.get("type") == "markets":
                for asset, info in data.get("markets", {}).items():
                    text = f"Asset {asset} trading at ${info['price']:.2f} with {info['change_24h']:.2f}% change"
                    samples.append(self.tokenize(text))
            
            elif data.get("type") == "tech" or data.get("type") == "discussion":
                for story in data.get("stories", []) or data.get("posts", []):
                    samples.append(self.tokenize(story))
            
            elif data.get("type") == "security":
                for cve in data.get("cves", []):
                    samples.append(self.tokenize(cve))
            
            elif data.get("type") == "research":
                for paper in data.get("papers", []):
                    samples.append(self.tokenize(paper))
            
            elif data.get("type") == "qa":
                for q in data.get("questions", []):
                    samples.append(self.tokenize(q))
        
        # Ø¥Ø°Ø§ ÙØ´Ù„Øª ÙƒÙ„ APIs
        if not samples:
            print(f"    âš ï¸ All APIs failed, using fallback")
            samples.append(self.tokenize(f"Training data for {self.layer_name}"))
        
        # ØªÙƒØ±Ø§Ø± Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ø­Ø¬Ù… ÙƒØ§ÙÙŠ
        while len(samples) < 50:
            samples.extend(samples[:5])
        
        self.data = samples[:50]
        print(f"    âœ… Got {len(self.data)} samples (Fetch #{self.fetch_count})")
        
        # ğŸ§¹ ØªÙ†Ø¸ÙŠÙ: Ø­Ø¯Ø¯ Ø­Ø¬Ù… Ø§Ù„Ù€ vocab (Ø§Ø­ØªÙØ¸ Ø¨Ø£Ø­Ø¯Ø« 5000 ÙƒÙ„Ù…Ø© ÙÙ‚Ø·)
        if len(self.vocab) > 5000:
            # Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹ (Ø£Ùˆ Ø£Ø­Ø¯Ø«Ù‡Ø§)
            self.vocab = dict(list(self.vocab.items())[-5000:])
            print(f"    ğŸ§¹ Cleaned vocab: {len(self.vocab)} words")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = torch.LongTensor(self.data[idx][:-1])
        y = torch.LongTensor(self.data[idx][1:])
        return x, y

class SmartTransformer(nn.Module):
    def __init__(self):
        super().__init__()
        d_model = 256
        self.embedding = nn.Embedding(10000, d_model)
        self.pos_encoder = nn.Embedding(512, d_model)
        encoder = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dim_feedforward=1024, dropout=0.1, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder, num_layers=4)
        self.fc = nn.Linear(d_model, 10000)
        self.d_model = d_model
    
    def forward(self, x):
        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)
        x = self.embedding(x) + self.pos_encoder(positions)
        x = self.transformer(x)
        return self.fc(x)

class RealAPITrainer:
    def __init__(self, name, desc):
        self.name = name
        self.desc = desc
        print(f"\nğŸ¯ [{name}]")
        print(f"    Purpose: {desc}")
        
        self.fetcher = RealAPIDataFetcher()
        self.model = SmartTransformer().to(DEVICE)
        self.dataset = RealDataset(name, self.fetcher)
        self.loader = DataLoader(self.dataset, batch_size=16, shuffle=True, num_workers=0)
        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.95)
        self.criterion = nn.CrossEntropyLoss()
        
        self.epoch = 0
        self.training = False
        self.stats = {
            "loss": 0.0,
            "accuracy": 0.0,
            "samples": 0,
            "api_fetches": 0
        }
        
        # Checkpoint directory
        self.checkpoint_dir = Path("learning_data/checkpoints") / name
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # Load existing checkpoint if available
        self._load_checkpoint()
    
    def train_epoch(self):
        """ØªØ¯Ø±ÙŠØ¨ epoch ÙˆØ§Ø­Ø¯"""
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ„ 5 epochs
        if self.epoch % 5 == 0:
            self.dataset.refresh()
        
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for data, target in self.loader:
            data, target = data.to(DEVICE), target.to(DEVICE)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output.view(-1, 10000), target.view(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            _, pred = output.max(2)
            correct += pred.eq(target).sum().item()
            total += target.numel()
            self.stats["samples"] += data.size(0)
        
        self.stats["loss"] = total_loss / len(self.loader)
        self.stats["accuracy"] = 100.0 * correct / total
        self.stats["api_fetches"] = self.dataset.fetch_count
        self.epoch += 1
        self.scheduler.step()
        
        if self.epoch % 10 == 0:
            print(f"  ğŸ“Š [{self.name}] Epoch {self.epoch:3d} | Loss: {self.stats['loss']:.4f} | Acc: {self.stats['accuracy']:.1f}% | Fetches: {self.stats['api_fetches']}")
        
        # Save checkpoint every 50 epochs
        if self.epoch % 50 == 0 and self.epoch > 0:
            self._save_checkpoint()
    
    def training_loop(self):
        print(f"ğŸ”¥ [{self.name}] Training with REAL API data...")
        while self.training:
            self.train_epoch()
            time.sleep(0.1)
    
    def start(self):
        if not self.training:
            self.training = True
            threading.Thread(target=self.training_loop, daemon=True).start()
    
    def stop(self):
        self.training = False
        self._save_checkpoint()
    
    def _save_checkpoint(self):
        """Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - Ù…Ø¹ rotation"""
        checkpoint = {
            'epoch': self.epoch,
            'model_state': self.model.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'scheduler_state': self.scheduler.state_dict(),
            'stats': {
                'loss': self.stats['loss'],
                'accuracy': self.stats['accuracy'],
                'samples': self.stats['samples'],
                'api_fetches': self.stats['api_fetches']
            },
            'vocab_size': len(self.dataset.vocab),
            'timestamp': datetime.now().isoformat()
        }
        
        # 1. Ø­ÙØ¸ latest.pt (Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù†Ø­ØªÙØ¸ Ø¨ÙŠ)
        path = self.checkpoint_dir / "latest.pt"
        torch.save(checkpoint, path)
        print(f"    ğŸ’¾ Saved checkpoint: {path} (Epoch {self.epoch})")
        
        # 2. Ø­ÙØ¸ best model (Ø¥Ø°Ø§ Ø¯Ù‚Ø© Ø¬ÙŠØ¯Ø©)
        if self.stats['accuracy'] > 90:
            best_path = self.checkpoint_dir / f"best_acc{self.stats['accuracy']:.1f}_ep{self.epoch}.pt"
            torch.save(checkpoint, best_path)
        
        # 3. ğŸ§¹ ROTATION: Ø§Ø­Ø°Ù Ø§Ù„Ù‚Ø¯ÙŠÙ… (Ø§Ø­ØªÙØ¸ Ø¨Ø¢Ø®Ø± 3 best ÙÙ‚Ø·)
        self._cleanup_old_checkpoints()
        
        # 4. Auto-sync to Windows
        self._sync_to_windows(path)
    
    def _cleanup_old_checkpoints(self):
        """Ø­Ø°Ù Ø§Ù„Ù‚Ø¯ÙŠÙ… - Ø§Ø­ØªÙØ¸ Ø¨Ù€ latest.pt + Ø¢Ø®Ø± 2 best"""
        best_files = sorted(self.checkpoint_dir.glob("best_acc*.pt"), 
                          key=lambda x: x.stat().st_mtime, reverse=True)
        
        # Ø§Ø­Ø°Ù Ø§Ù„Ù‚Ø¯ÙŠÙ… (Ø£ÙƒØ«Ø± Ù…Ù† 2)
        for old_file in best_files[2:]:
            old_file.unlink()
            print(f"    ğŸ—‘ï¸ Removed old checkpoint: {old_file.name}")
    
    def _sync_to_windows(self, checkpoint_path):
        """Ù…Ø²Ø§Ù…Ù†Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ù„Ù€ Windows"""
        try:
            import requests
            windows_api = os.getenv("WINDOWS_API", "http://192.168.68.109:8000")
            
            with open(checkpoint_path, "rb") as f:
                files = {"file": (checkpoint_path.name, f, "application/octet-stream")}
                response = requests.post(
                    f"{windows_api}/api/v1/checkpoints/upload/{self.name}",
                    files=files,
                    timeout=30
                )
                if response.status_code == 200:
                    print(f"    ğŸ”„ Auto-synced to Windows: {self.name}/{checkpoint_path.name}")
                else:
                    print(f"    âš ï¸ Auto-sync failed: HTTP {response.status_code}")
        except Exception as e:
            print(f"    âš ï¸ Auto-sync error: {e}")
    
    def _load_checkpoint(self):
        """Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        path = self.checkpoint_dir / "latest.pt"
        if path.exists():
            try:
                checkpoint = torch.load(path, map_location=DEVICE)
                self.model.load_state_dict(checkpoint['model_state'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state'])
                self.scheduler.load_state_dict(checkpoint['scheduler_state'])
                self.epoch = checkpoint['epoch']
                # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù€ stats
                saved_stats = checkpoint.get('stats', {})
                self.stats['loss'] = saved_stats.get('loss', 0.0)
                self.stats['accuracy'] = saved_stats.get('accuracy', 0.0)
                self.stats['samples'] = saved_stats.get('samples', 0)
                self.stats['api_fetches'] = saved_stats.get('api_fetches', 0)
                print(f"    ğŸ“‚ Loaded checkpoint: Epoch {self.epoch}, Acc: {self.stats['accuracy']:.1f}%")
            except Exception as e:
                print(f"    âš ï¸ Failed to load checkpoint: {e}")
        else:
            print(f"    ğŸ†• Starting fresh training")

class RealAPILearningSystem:
    def __init__(self):
        self.layers = {}
        self.running = False
        
        configs = [
            ("president", "Strategic Decisions - GitHub + Markets"),
            ("seventh_dimension", "Future Planning - HackerNews"),
            ("high_council", "Collective Wisdom - Reddit Discussions"),
            ("shadow_light", "Risk Analysis - CVE + Markets"),
            ("scouts", "Intelligence - arXiv Research"),
            ("meta_team", "Optimization - StackOverflow"),
            ("domain_experts", "Expert Knowledge - arXiv"),
            ("execution", "Task Execution - GitHub + Reddit"),
            ("meta_architect", "Architecture - HackerNews + GitHub"),
            ("builder_council", "Software Dev - GitHub + StackOverflow"),
            ("executive_controller", "Control - Markets + Reddit"),
            ("guardian", "Cybersecurity - CVE Database"),
            ("cosmic_bridge", "API Integration - All Sources"),
            ("eternity", "Knowledge - arXiv + HackerNews"),
            ("learning_core", "Self-Improvement - StackOverflow")
        ]
        
        print("="*80)
        print("ğŸŒ REAL API LEARNING SYSTEM - 15 LAYERS")
        print("="*80)
        print("ğŸ“¡ APIs:")
        print("   â€¢ GitHub API - Real trending repositories")
        print("   â€¢ CoinGecko API - Live crypto prices")
        print("   â€¢ HackerNews API - Tech news")
        print("   â€¢ Reddit API - Programming discussions")
        print("   â€¢ StackOverflow API - Real questions")
        print("   â€¢ CVE API - Security vulnerabilities")
        print("   â€¢ arXiv API - Research papers")
        print("="*80)
        
        for i, (name, desc) in enumerate(configs, 1):
            print(f"\n[{i:2d}/15] {name}")
            self.layers[name] = RealAPITrainer(name, desc)
            if torch.cuda.is_available():
                vram = torch.cuda.memory_allocated() / 1e9
                print(f"      ğŸ’¾ VRAM: {vram:.2f} GB")
        
        total_params = sum(sum(p.numel() for p in t.model.parameters()) for t in self.layers.values())
        print(f"\n{'='*80}")
        print(f"âœ… Total: {total_params/1e9:.2f}B Parameters")
        print(f"ğŸŒ All layers fetch REAL data from Internet APIs")
        print(f"{'='*80}\n")
    
    def start_all(self):
        print("ğŸš€ STARTING REAL API TRAINING...")
        print("ğŸŒ Fetching live data from GitHub, CoinGecko, HackerNews, Reddit, arXiv, CVE...\n")
        self.running = True
        for name, trainer in self.layers.items():
            trainer.start()
            time.sleep(0.5)
        print("\nğŸ”¥ ALL LAYERS LEARNING FROM REAL APIs!")
    
    def stop_all(self):
        self.running = False
        for t in self.layers.values():
            t.stop()
    
    def get_status(self):
        gpu = {}
        if torch.cuda.is_available():
            try:
                gpu = {
                    "name": torch.cuda.get_device_name(0),
                    "utilization": torch.cuda.utilization(),
                    "memory_used": torch.cuda.memory_allocated() / 1e9,
                    "memory_total": torch.cuda.get_device_properties(0).total_memory / 1e9
                }
            except:
                pass
        
        # Count saved checkpoints
        total_checkpoints = 0
        for t in self.layers.values():
            if t.checkpoint_dir.exists():
                total_checkpoints += len(list(t.checkpoint_dir.glob("*.pt")))
        
        return {
            "is_training": self.running,
            "device": str(DEVICE),
            "mode": "REAL_API_LEARNING",
            "apis": ["GitHub", "CoinGecko", "HackerNews", "Reddit", "StackOverflow", "CVE", "arXiv"],
            "gpu": gpu,
            "checkpoints": {
                "saved_total": total_checkpoints,
                "location": "learning_data/checkpoints/",
                "auto_save_every": 50
            },
            "layers": {
                name: {
                    "epoch": t.epoch,
                    "loss": t.stats["loss"],
                    "accuracy": t.stats["accuracy"],
                    "samples": t.stats["samples"],
                    "api_fetches": t.stats["api_fetches"],
                    "specialization": t.desc
                }
                for name, t in self.layers.items()
            }
        }

real_api_learning_system = RealAPILearningSystem()

if __name__ == "__main__":
    real_api_learning_system.start_all()
    try:
        while True:
            time.sleep(10)
    except KeyboardInterrupt:
        real_api_learning_system.stop_all()
