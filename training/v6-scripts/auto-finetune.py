"""
Bi IDE â€“ ØªØ¯Ø±ÙŠØ¨ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø§Ù„Ø®Ù„ÙÙŠØ©
Auto Fine-tuning â€“ ÙŠØ´ØªØºÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¹Ù†Ø¯Ù…Ø§ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± ÙØ§Ø¶ÙŠ

ÙŠØ¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù…Ø© Ù…Ù†:
- LearningCore (auto-training-data.json)
- ErrorLearner (error-training-data.json)
- ProjectLearner (training/output/auto_learned_training.json)
- ÙƒÙ„ Ù…Ù„ÙØ§Øª training/output/*.json

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  python training/auto-finetune.py              # ØªØ¯Ø±ÙŠØ¨ ÙÙˆØ±ÙŠ
  python training/auto-finetune.py --watch      # Ù…Ø±Ø§Ù‚Ø¨Ø© + ØªØ¯Ø±ÙŠØ¨ Ø¹Ù†Ø¯ idle
  python training/auto-finetune.py --check      # ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·
"""

import json
import os
import sys
import time
import glob
import psutil
from pathlib import Path
from datetime import datetime

BASE_DIR = Path(__file__).parent.parent
TRAINING_DIR = BASE_DIR / "training" / "output"
KNOWLEDGE_DIR = BASE_DIR / "data" / "knowledge"
MODELS_DIR = BASE_DIR / "models"
OUTPUT_DIR = MODELS_DIR / "finetuned"
MERGED_DIR = MODELS_DIR / "merged"
ONNX_DIR = MODELS_DIR / "bi-ai-onnx"
STATE_FILE = BASE_DIR / "data" / "learning" / "auto-finetune-state.json"

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
MODEL_NAME = "Qwen/Qwen2-0.5B"
MAX_LENGTH = 512
BATCH_SIZE = 2
GRAD_ACCUM = 8
EPOCHS = 5              # Ø£Ù‚Ù„ Ù…Ù† finetune-extended (ØªØ¯Ø±ÙŠØ¨ Ø³Ø±ÙŠØ¹)
LEARNING_RATE = 2e-4
LORA_R = 16
LORA_ALPHA = 32
MIN_DATA_COUNT = 200     # Ø£Ù‚Ù„ Ø¹Ø¯Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¨Ø¯Ø¡
MIN_IDLE_CPU = 40        # Ø£Ù‚ØµÙ‰ CPU% Ù„Ù„Ø¨Ø¯Ø¡
MIN_HOURS_BETWEEN = 12   # Ø£Ù‚Ù„ ÙØªØ±Ø© Ø¨ÙŠÙ† ØªØ¯Ø±ÙŠØ¨ÙŠÙ†

def load_state():
    """ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©"""
    if STATE_FILE.exists():
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"last_training": None, "total_trainings": 0, "total_samples": 0}

def save_state(state):
    """Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def collect_all_data():
    """Ø¬Ù…Ø¹ ÙƒÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† ÙƒÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø±"""
    all_data = []
    sources = {}
    
    # 1. Ù…Ù„ÙØ§Øª training/output/*.json
    if TRAINING_DIR.exists():
        for json_file in TRAINING_DIR.glob("*.json"):
            if json_file.name == "training_report.json":
                continue
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if isinstance(data, list):
                    valid = [d for d in data if d.get('instruction') and d.get('output')]
                    all_data.extend(valid)
                    sources[json_file.name] = len(valid)
            except:
                pass
    
    # 2. Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
    kb_files = ['auto-training-data.json', 'error-training-data.json', 'auto-learning-data.json']
    for kb_file in kb_files:
        kb_path = KNOWLEDGE_DIR / kb_file
        if kb_path.exists():
            try:
                with open(kb_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if isinstance(data, list):
                    valid = [d for d in data if d.get('instruction') and d.get('output')]
                    all_data.extend(valid)
                    sources[kb_file] = len(valid)
            except:
                pass
    
    # 3. RAG knowledge base
    rag_file = KNOWLEDGE_DIR / "rag-knowledge-base.json"
    if rag_file.exists():
        try:
            with open(rag_file, 'r', encoding='utf-8') as f:
                docs = json.load(f)
            for doc in docs:
                text = doc.get('text', '')
                answer = doc.get('answer', '')
                if text and answer:
                    all_data.append({
                        'instruction': text[:200],
                        'output': answer[:500]
                    })
            sources['rag-knowledge-base.json'] = len(docs)
        except:
            pass
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
    seen = set()
    unique_data = []
    for item in all_data:
        key = f"{item.get('instruction', '')[:50]}|{item.get('output', '')[:50]}"
        if key not in seen:
            seen.add(key)
            unique_data.append(item)
    
    return unique_data, sources

def check_conditions(state):
    """ÙØ­Øµ Ø´Ø±ÙˆØ· Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    issues = []
    
    # ÙØ­Øµ CPU
    cpu = psutil.cpu_percent(interval=2)
    if cpu > MIN_IDLE_CPU:
        issues.append(f"CPU Ø¹Ø§Ù„ÙŠ: {cpu}% (Ø§Ù„Ø­Ø¯: {MIN_IDLE_CPU}%)")
    
    # ÙØ­Øµ Ø¢Ø®Ø± ØªØ¯Ø±ÙŠØ¨
    if state.get('last_training'):
        last = datetime.fromisoformat(state['last_training'])
        hours_since = (datetime.now() - last).total_seconds() / 3600
        if hours_since < MIN_HOURS_BETWEEN:
            issues.append(f"Ø¢Ø®Ø± ØªØ¯Ø±ÙŠØ¨ Ù‚Ø¨Ù„ {hours_since:.1f} Ø³Ø§Ø¹Ø© (Ø§Ù„Ø­Ø¯: {MIN_HOURS_BETWEEN})")
    
    # ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    data, sources = collect_all_data()
    if len(data) < MIN_DATA_COUNT:
        issues.append(f"Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø©: {len(data)} (Ø§Ù„Ø­Ø¯: {MIN_DATA_COUNT})")
    
    return issues, data, sources

def format_data_for_training(data):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØµÙŠØºØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    formatted = []
    for item in data:
        instruction = item.get('instruction', '')
        input_text = item.get('input', '')
        output = item.get('output', '')
        
        if input_text:
            text = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
        else:
            text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"
        
        formatted.append({"text": text})
    
    return formatted

def train(data):
    """ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
        from peft import LoraConfig, get_peft_model, TaskType
        from datasets import Dataset
    except ImportError as e:
        print(f"âŒ Ù…ÙƒØªØ¨Ø§Øª Ù…ÙÙ‚ÙˆØ¯Ø©: {e}")
        print("   pip install torch transformers peft datasets")
        return False
    
    print(f"\nğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
        trust_remote_code=True
    )
    
    # LoRA
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    formatted = format_data_for_training(data)
    dataset = Dataset.from_list(formatted)
    
    def tokenize(example):
        result = tokenizer(example["text"], truncation=True, max_length=MAX_LENGTH, padding="max_length")
        result["labels"] = result["input_ids"].copy()
        return result
    
    dataset = dataset.map(tokenize, remove_columns=["text"])
    split = dataset.train_test_split(test_size=0.1, seed=42)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    training_args = TrainingArguments(
        output_dir=str(OUTPUT_DIR),
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LEARNING_RATE,
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,
        logging_steps=50,
        save_steps=500,
        eval_strategy="steps",
        eval_steps=500,
        save_total_limit=2,
        fp16=torch.cuda.is_available(),
        report_to="none",
        load_best_model_at_end=True,
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=split["train"],
        eval_dataset=split["test"],
    )
    
    print(f"\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {len(split['train'])} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨, {len(split['test'])} Ø¹ÙŠÙ†Ø© Ø§Ø®ØªØ¨Ø§Ø±")
    trainer.train()
    
    # Ø­ÙØ¸
    model.save_pretrained(str(OUTPUT_DIR))
    tokenizer.save_pretrained(str(OUTPUT_DIR))
    print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {OUTPUT_DIR}")
    
    return True

def merge_and_convert():
    """Ø¯Ù…Ø¬ LoRA ÙˆØªØ­ÙˆÙŠÙ„ Ù„Ù€ ONNX"""
    print("\nğŸ”— Ø¯Ù…Ø¬ LoRA Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ...")
    
    try:
        from peft import PeftModel
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
        base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)
        model = PeftModel.from_pretrained(base_model, str(OUTPUT_DIR))
        model = model.merge_and_unload()
        
        MERGED_DIR.mkdir(parents=True, exist_ok=True)
        model.save_pretrained(str(MERGED_DIR))
        tokenizer.save_pretrained(str(MERGED_DIR))
        print(f"âœ… ØªÙ… Ø§Ù„Ø¯Ù…Ø¬ ÙÙŠ: {MERGED_DIR}")
        
        # ØªØ­ÙˆÙŠÙ„ ONNX
        print("\nğŸ“¦ ØªØ­ÙˆÙŠÙ„ Ù„Ù€ ONNX...")
        try:
            from optimum.onnxruntime import ORTModelForCausalLM
            ort_model = ORTModelForCausalLM.from_pretrained(str(MERGED_DIR), export=True)
            ONNX_DIR.mkdir(parents=True, exist_ok=True)
            ort_model.save_pretrained(str(ONNX_DIR))
            tokenizer.save_pretrained(str(ONNX_DIR))
            print(f"âœ… ØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù„Ù€ ONNX ÙÙŠ: {ONNX_DIR}")
        except Exception as e:
            print(f"âš ï¸ ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ ONNX: {e}")
            print("   ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­ÙˆÙŠÙ„Ù‡ ÙŠØ¯ÙˆÙŠØ§Ù‹: python training/convert-to-onnx.py")
        
        return True
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø¯Ù…Ø¬: {e}")
        return False

def main():
    args = sys.argv[1:]
    state = load_state()
    
    # ÙØ­Øµ ÙÙ‚Ø·
    if '--check' in args:
        data, sources = collect_all_data()
        print(f"\nğŸ“Š Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªØ§Ø­Ø©: {len(data)} Ø¹ÙŠÙ†Ø©")
        print("\nØ§Ù„Ù…ØµØ§Ø¯Ø±:")
        for source, count in sorted(sources.items(), key=lambda x: -x[1]):
            print(f"  ğŸ“„ {source}: {count}")
        print(f"\nØ¢Ø®Ø± ØªØ¯Ø±ÙŠØ¨: {state.get('last_training', 'Ù„Ù… ÙŠØªÙ… Ø¨Ø¹Ø¯')}")
        print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨Ø§Øª: {state.get('total_trainings', 0)}")
        return
    
    # ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
    if '--watch' in args:
        print("ğŸ‘ï¸ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© - ÙŠØ±Ø§Ù‚Ø¨ ÙˆÙŠØªØ¯Ø±Ø¨ Ø¹Ù†Ø¯ idle...")
        while True:
            issues, data, sources = check_conditions(state)
            if issues:
                print(f"â³ [{datetime.now().strftime('%H:%M')}] Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨:")
                for issue in issues:
                    print(f"   - {issue}")
                time.sleep(300)  # ÙØ­Øµ ÙƒÙ„ 5 Ø¯Ù‚Ø§Ø¦Ù‚
                continue
            
            print(f"\nâœ… Ø§Ù„Ø´Ø±ÙˆØ· Ù…ØªØ­Ù‚Ù‚Ø©! Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ({len(data)} Ø¹ÙŠÙ†Ø©)...")
            success = train(data)
            if success:
                merge_and_convert()
                state['last_training'] = datetime.now().isoformat()
                state['total_trainings'] = state.get('total_trainings', 0) + 1
                state['total_samples'] = len(data)
                save_state(state)
            
            time.sleep(3600)  # Ø§Ù†ØªØ¸Ø§Ø± Ø³Ø§Ø¹Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        return
    
    # ØªØ¯Ø±ÙŠØ¨ ÙÙˆØ±ÙŠ
    print("ğŸš€ ØªØ¯Ø±ÙŠØ¨ ÙÙˆØ±ÙŠ...")
    data, sources = collect_all_data()
    print(f"ğŸ“Š Ø¨ÙŠØ§Ù†Ø§Øª: {len(data)} Ø¹ÙŠÙ†Ø© Ù…Ù† {len(sources)} Ù…ØµØ¯Ø±")
    
    if len(data) < 10:
        print("âŒ Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹! Ø£Ø¶Ù Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù„Ù„ØªØ¹Ù„Ù… Ø£ÙˆÙ„Ø§Ù‹.")
        return
    
    for source, count in sorted(sources.items(), key=lambda x: -x[1])[:5]:
        print(f"  ğŸ“„ {source}: {count}")
    
    success = train(data)
    if success:
        merge_and_convert()
        state['last_training'] = datetime.now().isoformat()
        state['total_trainings'] = state.get('total_trainings', 0) + 1
        state['total_samples'] = len(data)
        save_state(state)
        print("\nğŸ‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø¬Ø§Ø­!")

if __name__ == "__main__":
    main()
