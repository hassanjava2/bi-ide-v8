#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bi IDE â€“ Ø¯Ù…Ø¬ LoRA ÙˆØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¥Ù„Ù‰ GGUF (Q4_K_M)
1. ÙŠØ¯Ù…Ø¬ Ø£ÙˆØ²Ø§Ù† LoRA Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ØµÙ„ÙŠ
2. ÙŠØ­ÙˆÙ‘Ù„ Ø¥Ù„Ù‰ GGUF Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… convert_hf_to_gguf.py Ù…Ù† llama.cpp
3. ÙŠÙƒÙ…Ù‘Ù… Ø¥Ù„Ù‰ Q4_K_M Ø¥Ù† ÙˆÙØ¬Ø¯ ØªÙ†ÙÙŠØ°ÙŠ quantize

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    python training/convert-to-gguf.py
    python training/convert-to-gguf.py --merged-dir models/finetuned-chat-merged --out models/bi-chat-gguf
"""

import argparse
import os
import shutil
import subprocess
import sys
import urllib.request
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent
FINETUNED_CHAT = BASE_DIR / "models" / "finetuned-chat"
MERGED_DIR = BASE_DIR / "models" / "finetuned-chat-merged"
OUTPUT_DIR = BASE_DIR / "models" / "bi-chat-gguf"
CONVERT_SCRIPT = BASE_DIR / "training" / "convert_hf_to_gguf.py"
GGUF_SCRIPT_URL = "https://raw.githubusercontent.com/ggerganov/llama.cpp/master/convert_hf_to_gguf.py"


def merge_lora():
    """Ø¯Ù…Ø¬ Ø£ÙˆØ²Ø§Ù† LoRA Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬."""
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel

    model_name = "Qwen/Qwen2.5-3B-Instruct"
    if not FINETUNED_CHAT.exists() or not (FINETUNED_CHAT / "adapter_config.json").exists():
        print(f"âŒ Ù…Ø¬Ù„Ø¯ LoRA ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {FINETUNED_CHAT}")
        print("   Ø´ØºÙ‘Ù„ Ø£ÙˆÙ„Ø§Ù‹: python training/finetune-chat.py")
        sys.exit(1)

    print("ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ + LoRA...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,
        device_map=None,
        trust_remote_code=True,
    )
    model = PeftModel.from_pretrained(model, str(FINETUNED_CHAT))
    print("ğŸ”„ Ø¯Ù…Ø¬ LoRA Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ...")
    model = model.merge_and_unload()
    MERGED_DIR.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬ ÙÙŠ {MERGED_DIR}...")
    model.save_pretrained(MERGED_DIR, safe_serialization=True)
    tokenizer.save_pretrained(MERGED_DIR)
    print("âœ… ØªÙ… Ø§Ù„Ø¯Ù…Ø¬ ÙˆØ§Ù„Ø­ÙØ¸.")
    return MERGED_DIR


def ensure_convert_script():
    if CONVERT_SCRIPT.exists():
        return str(CONVERT_SCRIPT)
    print("ğŸ“¥ ØªÙ†Ø²ÙŠÙ„ convert_hf_to_gguf.py Ù…Ù† llama.cpp...")
    try:
        urllib.request.urlretrieve(GGUF_SCRIPT_URL, CONVERT_SCRIPT)
        return str(CONVERT_SCRIPT)
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø§Ù„ØªÙ†Ø²ÙŠÙ„: {e}")
        print("   Ø­Ù…Ù‘Ù„ ÙŠØ¯ÙˆÙŠØ§Ù‹ Ù…Ù†: https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py")
        sys.exit(1)


def run_convert_to_gguf(merged_dir, out_dir):
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    gguf_path = out_dir / "bi-chat-3b-f16.gguf"
    script = ensure_convert_script()
    cmd = [sys.executable, script, str(merged_dir), "--outfile", str(gguf_path), "--outtype", "f16"]
    print(f"ğŸ”„ ØªØ´ØºÙŠÙ„ ØªØ­ÙˆÙŠÙ„ GGUF: {' '.join(cmd)}")
    r = subprocess.run(cmd, cwd=str(BASE_DIR))
    if r.returncode != 0:
        print("âŒ ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ GGUF. ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª gguf: pip install gguf")
        sys.exit(1)
    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡: {gguf_path}")
    return gguf_path


def run_quantize(gguf_f16_path, out_dir):
    out_dir = Path(out_dir)
    q4_path = out_dir / "bi-chat-3b-q4.gguf"
    quantize_bin = shutil.which("quantize") or shutil.which("llama-quantize")
    if not quantize_bin:
        print("âš ï¸ ØªÙ†ÙÙŠØ°ÙŠ quantize ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ PATH.")
        print("   Ù„Ø¥Ù†Ø´Ø§Ø¡ Q4_K_M: Ø«Ø¨Ù‘Øª llama.cpp Ø«Ù… Ø´ØºÙ‘Ù„:")
        print(f"   quantize {gguf_f16_path} {q4_path} Q4_K_M")
        return None
    cmd = [quantize_bin, str(gguf_f16_path), str(q4_path), "Q4_K_M"]
    print(f"ğŸ”„ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙƒÙ…ÙŠÙ…: {' '.join(cmd)}")
    r = subprocess.run(cmd)
    if r.returncode != 0:
        print("âŒ ÙØ´Ù„ Ø§Ù„ØªÙƒÙ…ÙŠÙ….")
        return None
    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡: {q4_path}")
    return q4_path


def main():
    parser = argparse.ArgumentParser(description="Ø¯Ù…Ø¬ LoRA ÙˆØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ GGUF")
    parser.add_argument("--skip-merge", action="store_true", help="ØªØ®Ø·ÙŠ Ø§Ù„Ø¯Ù…Ø¬ (Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù„Ø¯ merged Ù…ÙˆØ¬ÙˆØ¯)")
    parser.add_argument("--merged-dir", type=str, default=str(MERGED_DIR))
    parser.add_argument("--out", type=str, default=str(OUTPUT_DIR))
    args = parser.parse_args()

    print("=" * 50)
    print("Bi IDE â€“ ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¥Ù„Ù‰ GGUF")
    print("=" * 50)

    if args.skip_merge and Path(args.merged_dir).exists():
        merged_dir = Path(args.merged_dir)
        print(f"Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù„Ø¯ Ù…Ø¯Ù…Ø¬ Ù…ÙˆØ¬ÙˆØ¯: {merged_dir}")
    else:
        merged_dir = merge_lora()

    gguf_f16 = run_convert_to_gguf(merged_dir, args.out)
    run_quantize(gguf_f16, args.out)
    print("\nâœ… Ø§Ù†ØªÙ‡Ù‰. Ø§Ù†Ø³Ø® Ù…Ù„Ù bi-chat-3b-q4.gguf (Ø£Ùˆ f16) Ø¥Ù„Ù‰ models/ ÙˆØ§Ø³ØªØ®Ø¯Ù…Ù‡ ÙÙŠ Bi IDE.")


if __name__ == "__main__":
    main()
