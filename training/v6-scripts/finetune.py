"""
Bi IDE â€“ Fine-tuning Script
ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ AI Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø§Ù„Ø®Ø§ØµØ©

Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:
    pip install torch transformers datasets peft accelerate

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    python training/finetune.py

RTX 3060 (6GB VRAM) â†’ LoRA Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ ØµØºÙŠØ±
"""

import json
import os
import sys
from pathlib import Path

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BASE_DIR = Path(__file__).parent.parent
TRAINING_DIR = BASE_DIR / "training" / "output"
KNOWLEDGE_FILE = BASE_DIR / "data" / "knowledge" / "rag-knowledge-base.json"
OUTPUT_DIR = BASE_DIR / "models" / "finetuned"

# Ù†Ù…ÙˆØ°Ø¬ ØµØºÙŠØ± - Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ø³ÙŠØ±ÙØ± (8 cores, 31GB RAM)
# Ù…Ù„Ø§Ø­Ø¸Ø©: Ù†Ø®Ù„ÙŠÙ‡Ø§ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¹Ø¯ÙŠÙ„ Ù…Ù† Ø§Ù„Ø¨ÙŠØ¦Ø© Ø­ØªÙ‰ Ù†Ù‚Ø¯Ø± Ù†Ø·Ø¨Ù‘Ù‚ Ø®Ø·Ø© Ù†Ø¨Ø¶Ø§Øª Hostinger (ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¶ØºØ·).
MODEL_NAME = os.environ.get("MODEL_NAME", "Qwen/Qwen2-0.5B")
MAX_LENGTH = int(os.environ.get("MAX_LENGTH", "512"))
BATCH_SIZE = int(os.environ.get("BATCH_SIZE", "16"))          # Ø£ÙƒØ¨Ø± = Ø£Ø³Ø±Ø¹
EPOCHS = float(os.environ.get("EPOCHS", "3"))
LEARNING_RATE = float(os.environ.get("LEARNING_RATE", "2e-4"))
LORA_R = int(os.environ.get("LORA_R", "16"))
LORA_ALPHA = int(os.environ.get("LORA_ALPHA", "32"))
NUM_WORKERS = int(os.environ.get("NUM_WORKERS", "6"))          # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
CPU_THREADS = int(os.environ.get("CPU_THREADS", "8"))          # Ø¹Ø¯Ø¯ Ø®ÙŠÙˆØ· Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬

# Ø®Ø·Ø© Ù†Ø¨Ø¶Ø§Øª: Ø¥ÙŠÙ‚Ø§Ù ØªØ¯Ø±ÙŠØ¬ÙŠ Ø¨Ø¹Ø¯ Ù…Ø¯Ø© Ù…Ø­Ø¯Ø¯Ø© (Ø¯Ù‚Ø§Ø¦Ù‚) Ù„Ø­ÙØ¸ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Hostinger
PULSE_MAX_MINUTES = int(os.environ.get("PULSE_MAX_MINUTES", "0"))

# ØªØ¯Ø±ÙŠØ¨ Ø¹Ù…Ù„Ø§Ù‚: Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ø®Ø·ÙˆØ§Øª (ÙŠÙØ³ØªØ®Ø¯Ù… Ù„ØªÙ…Ø¯ÙŠØ¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø¯Ù„ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ epochs ÙÙ‚Ø·)
MAX_STEPS = int(os.environ.get("MAX_STEPS", "0"))
SAVE_STEPS = int(os.environ.get("SAVE_STEPS", "500"))
EVAL_STEPS = int(os.environ.get("EVAL_STEPS", "500"))

def check_dependencies():
    """ÙØ­Øµ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
    missing = []
    for lib in ['torch', 'transformers', 'datasets', 'peft', 'accelerate']:
        try:
            __import__(lib)
        except ImportError:
            missing.append(lib)
    
    if missing:
        print(f"âŒ Ù…ÙƒØªØ¨Ø§Øª Ù†Ø§Ù‚ØµØ©: {', '.join(missing)}")
        print(f"   Ø«Ø¨Ù‘ØªÙ‡Ø§ Ø¨Ù€: pip install {' '.join(missing)}")
        sys.exit(1)
    
    import torch
    print(f"âœ… PyTorch {torch.__version__}")
    print(f"   CUDA: {'âœ… ' + torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'âŒ CPU ÙÙ‚Ø·'}")
    if torch.cuda.is_available():
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   VRAM: {vram:.1f} GB")

def load_training_data():
    """ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    all_data = []
    
    # 1. Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
    if KNOWLEDGE_FILE.exists():
        print(f"ğŸ“š ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©...")
        with open(KNOWLEDGE_FILE, 'r', encoding='utf-8') as f:
            docs = json.load(f)
        for doc in docs:
            text = doc.get('text', '')
            answer = doc.get('answer', '')
            if text and answer:
                all_data.append({
                    "instruction": text,
                    "output": answer[:MAX_LENGTH * 2]
                })
        print(f"   âœ… {len(docs)} Ù…Ø³ØªÙ†Ø¯")
    
    # 2. Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    if TRAINING_DIR.exists():
        for f in sorted(TRAINING_DIR.glob("*.json")):
            try:
                with open(f, 'r', encoding='utf-8') as fh:
                    content = json.load(fh)
                if isinstance(content, list):
                    count = 0
                    for item in content:
                        inp = item.get('instruction', item.get('input', item.get('question', '')))
                        out = item.get('output', item.get('answer', item.get('response', '')))
                        if inp and out:
                            all_data.append({
                                "instruction": str(inp)[:MAX_LENGTH],
                                "output": str(out)[:MAX_LENGTH * 2]
                            })
                            count += 1
                    if count > 0:
                        print(f"   ğŸ“„ {f.name}: {count} Ø¹ÙŠÙ†Ø©")
            except Exception as e:
                print(f"   âš ï¸ {f.name}: {e}")
    
    print(f"\nğŸ“Š Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹: {len(all_data)} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨")
    return all_data

def format_for_training(data):
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØµÙŠØºØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    formatted = []
    for item in data:
        text = f"### Ø³Ø¤Ø§Ù„:\n{item['instruction']}\n\n### Ø¬ÙˆØ§Ø¨:\n{item['output']}"
        formatted.append({"text": text})
    return formatted

def train():
    """Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ"""
    import torch
    from transformers import (
        AutoModelForCausalLM, AutoTokenizer,
        TrainingArguments, Trainer, DataCollatorForLanguageModeling
    )
    from transformers import TrainerCallback
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    
    # Ø§Ø³ØªØºÙ„Ø§Ù„ Ø¹Ø¯Ø¯ Ø®ÙŠÙˆØ· Ù…Ø­Ø¯Ø¯ (Ù…Ù‡Ù… Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø¶ØºØ·)
    torch.set_num_threads(CPU_THREADS)
    torch.set_num_interop_threads(CPU_THREADS)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ–¥ï¸ Ø§Ù„Ø¬Ù‡Ø§Ø²: {device}")
    print(f"   CPU threads: {CPU_THREADS} | Data workers: {NUM_WORKERS}")
    
    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print("\nğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    raw_data = load_training_data()
    if len(raw_data) < 10:
        print("âŒ Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ (Ø£Ù‚Ù„ Ù…Ù† 10). Ø£Ø¶Ù Ø§Ù„Ù…Ø²ÙŠØ¯.")
        return
    
    formatted = format_for_training(raw_data)
    dataset = Dataset.from_list(formatted)
    
    # ØªÙ‚Ø³ÙŠÙ… train/eval
    split = dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = split['train']
    eval_dataset = split['test']
    print(f"   Train: {len(train_dataset)} | Eval: {len(eval_dataset)}")
    
    # 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print(f"\nğŸ¤– ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True
    )
    
    # 3. Ø¥Ø¹Ø¯Ø§Ø¯ LoRA
    print("ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯ LoRA...")
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
    )
    
    model = get_peft_model(model, lora_config)
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"   Parameters: {trainable:,} trainable / {total:,} total ({trainable/total*100:.1f}%)")
    
    # 4. Tokenize
    print("ğŸ”¤ Tokenizing...")
    def tokenize(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=MAX_LENGTH,
            padding="max_length"
        )
    
    train_tokenized = train_dataset.map(tokenize, batched=True, remove_columns=["text"])
    eval_tokenized = eval_dataset.map(tokenize, batched=True, remove_columns=["text"])
    
    # 5. ØªØ¯Ø±ÙŠØ¨
    output_dir = str(OUTPUT_DIR)
    os.makedirs(output_dir, exist_ok=True)

    class StopAfterTimeCallback(TrainerCallback):
        def __init__(self, max_minutes):
            self.max_minutes = max_minutes
            self.start = None

        def on_train_begin(self, args, state, control, **kwargs):
            import time
            self.start = time.time()
            return control

        def on_step_end(self, args, state, control, **kwargs):
            if not self.max_minutes or self.max_minutes <= 0:
                return control
            import time
            elapsed_min = (time.time() - (self.start or time.time())) / 60
            if elapsed_min >= self.max_minutes:
                # Ø¥ÙŠÙ‚Ø§Ù ØªØ¯Ø±ÙŠØ¬ÙŠ + Ø·Ù„Ø¨ Ø­ÙØ¸ checkpoint
                control.should_training_stop = True
                control.should_save = True
            return control
    
    # Ø¥Ø°Ø§ MAX_STEPS Ù…ÙØ¹Ù‘Ù„ØŒ Ù†ÙØ¶Ù‘Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© save/eval Ø¨Ø§Ù„Ø®Ø·ÙˆØ§Øª
    use_max_steps = isinstance(MAX_STEPS, int) and MAX_STEPS > 0

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=EPOCHS if not use_max_steps else 1,
        max_steps=MAX_STEPS if use_max_steps else -1,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=2,
        learning_rate=LEARNING_RATE,
        fp16=device == "cuda",
        logging_steps=10,
        eval_strategy="steps" if use_max_steps else "epoch",
        eval_steps=EVAL_STEPS if use_max_steps else None,
        save_strategy="steps" if use_max_steps else "epoch",
        save_steps=SAVE_STEPS if use_max_steps else None,
        save_total_limit=2,
        load_best_model_at_end=True,
        report_to="none",
        warmup_ratio=0.1,
        dataloader_num_workers=NUM_WORKERS,
        dataloader_pin_memory=False,
    )
    
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tokenized,
        eval_dataset=eval_tokenized,
        data_collator=data_collator,
        callbacks=[StopAfterTimeCallback(PULSE_MAX_MINUTES)] if PULSE_MAX_MINUTES > 0 else None,
    )
    
    print(f"\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ({EPOCHS} epochs, batch={BATCH_SIZE})...")
    print("=" * 50)
    
    trainer.train()
    
    # 6. Ø­ÙØ¸
    print("\nğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    print(f"\nâœ… ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"   ğŸ“ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {output_dir}")
    print(f"   ğŸ“Š Ø¹ÙŠÙ†Ø§Øª: {len(train_dataset)}")
    print(f"   ğŸ”„ Epochs: {EPOCHS}")
    print(f"\nğŸ’¡ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©: ØªØ­ÙˆÙŠÙ„ Ù„Ù€ ONNX Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Bi IDE")

if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ§  Bi IDE â€“ Fine-tuning")
    print("=" * 50)
    
    check_dependencies()
    
    print()
    train()
