"""
Bi IDE â€“ ØªØ¯Ø±ÙŠØ¨ Ù…ÙƒØ«Ù (10 Ø³Ø§Ø¹Ø§Øª)
Extended Fine-tuning â€“ ÙŠØ³ØªØºÙ„ ÙƒÙ„ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ù„Ø£Ù‚ØµÙ‰ Ø­Ø¯

Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª: Ù†ÙØ³ finetune.py
Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: python training/finetune-extended.py

RTX 3060 Ã— 10 Ø³Ø§Ø¹Ø§Øª = ØªØ¯Ø±ÙŠØ¨ Ø¹Ù…ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹
"""

import json
import os
import sys
import time
import random
from pathlib import Path
from datetime import datetime, timedelta

BASE_DIR = Path(__file__).parent.parent
TRAINING_DIR = BASE_DIR / "training" / "output"
KNOWLEDGE_FILE = BASE_DIR / "data" / "knowledge" / "rag-knowledge-base.json"
OUTPUT_DIR = BASE_DIR / "models" / "finetuned-extended"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª â€“ Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù€ 10 Ø³Ø§Ø¹Ø§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MODEL_NAME = "Qwen/Qwen2-0.5B"
MAX_LENGTH = 768          # Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ø³Ø§Ø¨Ù‚
BATCH_SIZE = 2            # Ø£ØµØºØ± = Ø°Ø§ÙƒØ±Ø© Ø£Ù‚Ù„ = ØªØ¹Ù„Ù… Ø£Ø¯Ù‚
GRAD_ACCUM = 8            # ØªØ¹ÙˆÙŠØ¶ Ø§Ù„Ù€ batch Ø§Ù„ØµØºÙŠØ±
EPOCHS = 15               # Ø£ÙƒØ«Ø± Ø¨ÙƒØ«ÙŠØ±
LEARNING_RATE = 1e-4      # Ø£Ø¨Ø·Ø£ = ØªØ¹Ù„Ù… Ø£Ø¯Ù‚
WARMUP_RATIO = 0.05
LORA_R = 32               # Ø£ÙƒØ¨Ø± = Ù‚Ø¯Ø±Ø© ØªØ¹Ù„Ù… Ø£Ø¹Ù„Ù‰
LORA_ALPHA = 64
SAVE_EVERY_HOURS = 2      # Ø­ÙØ¸ ÙƒÙ„ Ø³Ø§Ø¹ØªÙŠÙ†

def load_all_data():
    """ØªØ­Ù…ÙŠÙ„ ÙˆØªÙˆØ³ÙŠØ¹ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    all_data = []
    
    # 1. Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
    if KNOWLEDGE_FILE.exists():
        with open(KNOWLEDGE_FILE, 'r', encoding='utf-8') as f:
            docs = json.load(f)
        for doc in docs:
            text = doc.get('text', '')
            answer = doc.get('answer', '')
            if text and answer:
                # Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
                all_data.append({"instruction": text, "output": answer})
                # Ù†Ø³Ø®Ø© Ù…Ø¹ÙƒÙˆØ³Ø© (Ø§Ù„Ø¬ÙˆØ§Ø¨ ÙƒØ³ÙŠØ§Ù‚ ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ ÙƒÙ…Ø®Ø±Ø¬)
                all_data.append({"instruction": f"Ù„Ø®Ù‘Øµ Ø§Ù„Ù…ÙÙ‡ÙˆÙ… Ø§Ù„ØªØ§Ù„ÙŠ:\n{answer[:300]}", "output": text})
        print(f"  ğŸ“š Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ©: {len(docs)} â†’ {len(all_data)} (Ù…Ø¹ ØªÙˆØ³ÙŠØ¹)")
    
    # 2. ÙƒÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    if TRAINING_DIR.exists():
        for f in sorted(TRAINING_DIR.glob("*.json")):
            try:
                with open(f, 'r', encoding='utf-8') as fh:
                    content = json.load(fh)
                if isinstance(content, list):
                    count = 0
                    for item in content:
                        inp = item.get('instruction', item.get('input', item.get('question', '')))
                        out = item.get('output', item.get('answer', item.get('response', '')))
                        if inp and out:
                            all_data.append({"instruction": str(inp)[:MAX_LENGTH], "output": str(out)[:MAX_LENGTH]})
                            count += 1
                    if count > 0:
                        print(f"  ğŸ“„ {f.name}: {count}")
            except:
                pass
    
    # 3. ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© (Data Augmentation)
    augmented = augment_data(all_data)
    all_data.extend(augmented)
    
    print(f"\nğŸ“Š Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„ÙƒÙ„ÙŠ: {len(all_data)} Ø¹ÙŠÙ†Ø©")
    return all_data

def augment_data(data):
    """ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª â€“ ØªÙˆÙ„ÙŠØ¯ Ø¹ÙŠÙ†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"""
    augmented = []
    print("\nğŸ”„ ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Augmentation)...")
    
    for item in data:
        inp = item['instruction']
        out = item['output']
        
        # 1. Ø³Ø¤Ø§Ù„ "Ù„Ù…Ø§Ø°Ø§" 
        if len(out) > 100:
            augmented.append({
                "instruction": f"Ù„Ù…Ø§Ø°Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨:\n{out[:200]}",
                "output": f"Ù†Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ù„Ø£Ù†Ù‡:\n{inp}\n\nØ§Ù„ØªÙØ§ØµÙŠÙ„:\n{out[:300]}"
            })
        
        # 2. Ø³Ø¤Ø§Ù„ "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„"
        if 'function' in out.lower() or 'def ' in out.lower() or 'class' in out.lower():
            augmented.append({
                "instruction": f"Ù…Ø§ Ù‡ÙŠ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª Ù„Ù€: {inp[:100]}",
                "output": f"Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª:\n1. {out[:200]}\n2. ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n3. Ø£Ø¶Ù ØªÙˆØ«ÙŠÙ‚Ø§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹\n4. Ø§ÙƒØªØ¨ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"
            })
        
        # 3. Ø³Ø¤Ø§Ù„ "Ø§Ø´Ø±Ø­ Ø§Ù„ÙƒÙˆØ¯"
        if '```' in out or 'function' in out or 'const ' in out:
            augmented.append({
                "instruction": f"Ø§Ø´Ø±Ø­ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø§Ù„ØªÙØµÙŠÙ„:\n{out[:300]}",
                "output": f"Ø´Ø±Ø­ Ø§Ù„ÙƒÙˆØ¯:\n\nÙ‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙŠÙ‚ÙˆÙ… Ø¨Ù€: {inp}\n\nØ§Ù„ØªÙØ§ØµÙŠÙ„:\n{out[:400]}"
            })
    
    # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„ØªÙˆØ³ÙŠØ¹
    if len(augmented) > len(data):
        augmented = random.sample(augmented, len(data))
    
    print(f"  âœ… {len(augmented)} Ø¹ÙŠÙ†Ø© Ù…ÙÙˆÙ„Ù‘Ø¯Ø©")
    return augmented

def format_conversations(data):
    """ØªØ­ÙˆÙŠÙ„ Ù„ØµÙŠØºØ© Ù…Ø­Ø§Ø¯Ø«Ø©"""
    formatted = []
    for item in data:
        # ØµÙŠØºØ© 1: Ø³Ø¤Ø§Ù„ ÙˆØ¬ÙˆØ§Ø¨
        formatted.append({"text": f"### Ø³Ø¤Ø§Ù„:\n{item['instruction']}\n\n### Ø¬ÙˆØ§Ø¨:\n{item['output']}"})
        
        # ØµÙŠØºØ© 2: ØªØ¹Ù„ÙŠÙ…Ø© ÙˆÙ…Ø®Ø±Ø¬ (Ù„Ø¨Ø¹Ø¶ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª)
        if random.random() < 0.3:
            formatted.append({"text": f"[INST] {item['instruction']} [/INST]\n{item['output']}"})
    
    random.shuffle(formatted)
    return formatted

def train():
    import torch
    from transformers import (
        AutoModelForCausalLM, AutoTokenizer,
        TrainingArguments, Trainer, DataCollatorForLanguageModeling,
        TrainerCallback
    )
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"\nğŸ–¥ï¸  Ø§Ù„Ø¬Ù‡Ø§Ø²: {device}")
    if device == "cuda":
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {vram:.1f} GB")
    
    start_time = time.time()
    end_time = start_time + (10 * 3600)  # 10 Ø³Ø§Ø¹Ø§Øª
    print(f"   â° Ø§Ù„Ø¨Ø¯Ø¡: {datetime.now().strftime('%H:%M')}")
    print(f"   â° Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: {(datetime.now() + timedelta(hours=10)).strftime('%H:%M')}")
    
    # 1. Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print("\nğŸ“¦ ØªØ­Ù…ÙŠÙ„ ÙˆØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    raw_data = load_all_data()
    formatted = format_conversations(raw_data)
    dataset = Dataset.from_list(formatted)
    
    split = dataset.train_test_split(test_size=0.05, seed=42)
    train_ds = split['train']
    eval_ds = split['test']
    print(f"   Train: {len(train_ds)} | Eval: {len(eval_ds)}")
    
    # 2. Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ â€“ Ø¨Ø¯ÙˆÙ† device_map="auto" Ù„Ø¶Ù…Ø§Ù† Ø­ÙØ¸ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
    print(f"\nğŸ¤– ØªØ­Ù…ÙŠÙ„: {MODEL_NAME}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        trust_remote_code=True
    )
    
    # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ù€ GPU ÙŠØ¯ÙˆÙŠØ§Ù‹
    if device == "cuda":
        model = model.to(device)
    
    # 3. LoRA Ø£ÙƒØ¨Ø±
    print("ğŸ”§ Ø¥Ø¹Ø¯Ø§Ø¯ LoRA (extended)...")
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    
    model = get_peft_model(model, lora_config)
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"   Parameters: {trainable:,} trainable / {total:,} total ({trainable/total*100:.1f}%)")
    
    # 4. Tokenize
    print("ğŸ”¤ Tokenizing...")
    def tokenize(examples):
        return tokenizer(examples["text"], truncation=True, max_length=MAX_LENGTH, padding="max_length")
    
    train_tok = train_ds.map(tokenize, batched=True, remove_columns=["text"])
    eval_tok = eval_ds.map(tokenize, batched=True, remove_columns=["text"])
    
    # 5. Callback Ù„Ø­ÙØ¸ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù…Ø¹ ÙƒÙ„ checkpoint
    class SavePeftModelCallback(TrainerCallback):
        """ÙŠØ¶Ù…Ù† Ø­ÙØ¸ Ø£ÙˆØ²Ø§Ù† LoRA Ù…Ø¹ ÙƒÙ„ checkpoint"""
        def on_save(self, args, state, control, **kwargs):
            checkpoint_dir = os.path.join(args.output_dir, f"checkpoint-{state.global_step}")
            if os.path.isdir(checkpoint_dir):
                # Ø­ÙØ¸ Ø£ÙˆØ²Ø§Ù† LoRA Ø¨ØµÙŠØºØ© safetensors
                kwargs['model'].save_pretrained(checkpoint_dir, safe_serialization=True)
                print(f"\n   ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø£ÙˆØ²Ø§Ù† LoRA ÙÙŠ: checkpoint-{state.global_step}")
            return control
    
    # 6. ØªØ¯Ø±ÙŠØ¨
    output = str(OUTPUT_DIR)
    os.makedirs(output, exist_ok=True)
    
    training_args = TrainingArguments(
        output_dir=output,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        learning_rate=LEARNING_RATE,
        lr_scheduler_type="cosine",
        fp16=device == "cuda",
        logging_steps=25,
        eval_strategy="steps",
        eval_steps=500,
        save_strategy="steps",
        save_steps=500,
        save_total_limit=5,
        save_safetensors=True,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        report_to="none",
        warmup_ratio=WARMUP_RATIO,
        weight_decay=0.01,
        max_grad_norm=1.0,
        dataloader_num_workers=0,
    )
    
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tok,
        eval_dataset=eval_tok,
        data_collator=data_collator,
        callbacks=[SavePeftModelCallback()],
    )
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ ØªØ¯Ø±ÙŠØ¨ Ù…ÙƒØ«Ù â€“ {EPOCHS} epochs")
    print(f"   Batch: {BATCH_SIZE} Ã— {GRAD_ACCUM} accum = {BATCH_SIZE * GRAD_ACCUM} effective")
    print(f"   LoRA r={LORA_R}, alpha={LORA_ALPHA}")
    print(f"   Max length: {MAX_LENGTH}")
    print(f"   LR: {LEARNING_RATE} (cosine)")
    print(f"{'='*60}\n")
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† checkpoint Ø³Ø§Ø¨Ù‚ ØµØ§Ù„Ø­ Ù„Ù„Ø§Ø³ØªÙƒÙ…Ø§Ù„
    last_checkpoint = None
    if os.path.isdir(output):
        checkpoints = []
        for d in os.listdir(output):
            cp_path = os.path.join(output, d)
            if d.startswith("checkpoint-") and os.path.isdir(cp_path):
                # ØªØ­Ù‚Ù‚ Ø£Ù† Ø§Ù„Ù€ checkpoint ÙŠØ­ØªÙˆÙŠ Ø£ÙˆØ²Ø§Ù† ÙØ¹Ù„ÙŠØ©
                has_weights = any(
                    f.endswith('.safetensors') or f.endswith('.bin')
                    for f in os.listdir(cp_path)
                )
                if has_weights:
                    checkpoints.append(cp_path)
                else:
                    print(f"   âš ï¸ {d} Ø¨Ø¯ÙˆÙ† Ø£ÙˆØ²Ø§Ù† - ÙŠØªÙ… ØªØ¬Ø§ÙˆØ²Ù‡")
        
        if checkpoints:
            last_checkpoint = max(checkpoints, key=lambda x: int(x.split("-")[-1]))
            print(f"\nğŸ”„ Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù†: {os.path.basename(last_checkpoint)}")
            print(f"   (Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ø³ØªÙÙƒÙ…ÙÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹)")
    
    if last_checkpoint:
        trainer.train(resume_from_checkpoint=last_checkpoint)
    else:
        if any(
            d.startswith("checkpoint-") 
            for d in os.listdir(output) 
            if os.path.isdir(os.path.join(output, d))
        ):
            print("\nâš ï¸  ÙˆÙØ¬Ø¯Øª checkpoints Ù‚Ø¯ÙŠÙ…Ø© Ø¨Ø¯ÙˆÙ† Ø£ÙˆØ²Ø§Ù† - ÙŠØªÙ… Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯")
            print("   (Ù‡Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø±Ø§Ø­ ØªÙ†Ø­ÙØ¸ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ù…Ø¹ ÙƒÙ„ checkpoint)")
        trainer.train()
    
    # 7. Ø­ÙØ¸ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    elapsed = (time.time() - start_time) / 3600
    print(f"\nğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ... (Ø¨Ø¹Ø¯ {elapsed:.1f} Ø³Ø§Ø¹Ø©)")
    model.save_pretrained(output, safe_serialization=True)
    tokenizer.save_pretrained(output)
    
    # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­ÙØ¸
    saved_files = [f for f in os.listdir(output) if f.endswith('.safetensors')]
    if saved_files:
        print(f"   âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸ Ø¨Ù†Ø¬Ø§Ø­: {saved_files}")
    else:
        # Ø­Ø§ÙˆÙ„ Ø§Ù„Ø­ÙØ¸ Ø¨ØµÙŠØºØ© bin ÙƒØ§Ø­ØªÙŠØ§Ø·
        print("   âš ï¸ safetensors Ù„Ù… ØªÙ†Ø­ÙØ¸ØŒ Ù†Ø­Ø§ÙˆÙ„ bin...")
        model.save_pretrained(output, safe_serialization=False)
        saved_bin = [f for f in os.listdir(output) if f.endswith('.bin')]
        if saved_bin:
            print(f"   âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸: {saved_bin}")
        else:
            print("   âŒ ÙØ´Ù„ Ø§Ù„Ø­ÙØ¸! Ø­Ø§ÙˆÙ„ ÙŠØ¯ÙˆÙŠØ§Ù‹")
    
    # 8. ØªÙ‚Ø±ÙŠØ±
    print(f"\n{'='*60}")
    print(f"âœ… ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙƒØ«Ù!")
    print(f"   ğŸ“ {output}")
    print(f"   ğŸ“Š Ø¹ÙŠÙ†Ø§Øª: {len(train_ds)}")
    print(f"   ğŸ”„ Epochs: {EPOCHS}")
    print(f"   â±ï¸  Ø§Ù„Ù…Ø¯Ø©: {elapsed:.1f} Ø³Ø§Ø¹Ø©")
    print(f"   ğŸ§  LoRA: r={LORA_R}, target=7 layers")
    print(f"{'='*60}")
    print(f"\nğŸ’¡ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ø´ØºÙ‘Ù„: python training/convert-to-onnx.py")

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§  Bi IDE â€“ ØªØ¯Ø±ÙŠØ¨ Ù…ÙƒØ«Ù (Extended Fine-tuning)")
    print("   Ù…ØµÙ…Ù… Ù„Ù€ 10 Ø³Ø§Ø¹Ø§Øª Ù…ØªÙˆØ§ØµÙ„Ø©")
    print("=" * 60)
    
    # ÙØ­Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
    for lib in ['torch', 'transformers', 'datasets', 'peft', 'accelerate']:
        try:
            __import__(lib)
        except ImportError:
            print(f"âŒ Ù†Ø§Ù‚Øµ: {lib}")
            sys.exit(1)
    
    import torch
    print(f"\nâœ… PyTorch {torch.__version__}")
    if torch.cuda.is_available():
        print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
    
    train()
