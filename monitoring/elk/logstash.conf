# BI-IDE v8 Logstash Pipeline Configuration
# Processes logs from Filebeat and sends to Elasticsearch

input {
  beats {
    port => 5044
    host => "0.0.0.0"
    ssl => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify_mode => "peer"
    ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
    congestion_threshold => 40
    client_inactivity_timeout => 60
  }

  # Direct TCP input for testing
  tcp {
    port => 5000
    codec => json_lines
  }

  # Syslog input
  syslog {
    port => 5140
    type => "syslog"
  }
}

filter {
  # Add common fields
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "logs"
      "[@metadata][ingest_time]" => "%{+YYYY.MM.dd}"
    }
  }

  # Parse JSON logs
  if [message] =~ "^\\{" {
    json {
      source => "message"
      target => "parsed"
      skip_on_invalid_json => true
      tag_on_failure => ["_jsonparsefailure"]
    }

    if "_jsonparsefailure" not in [tags] {
      mutate {
        rename => {
          "[parsed][timestamp]" => "[@timestamp]"
          "[parsed][level]" => "[log][level]"
          "[parsed][logger]" => "[log][logger]"
          "[parsed][message]" => "[log][message]"
          "[parsed][request_id]" => "[trace][id]"
          "[parsed][user_id]" => "[user][id]"
          "[parsed][session_id]" => "[session][id]"
          "[parsed][method]" => "[http][request][method]"
          "[parsed][path]" => "[http][request][path]"
          "[parsed][status_code]" => "[http][response][status_code]"
          "[parsed][duration_ms]" => "[event][duration]"
          "[parsed][client_ip]" => "[client][ip]"
          "[parsed][user_agent]" => "[user_agent][original]"
        }
        remove_field => ["parsed", "message"]
      }
    }
  }

  # Parse BI-IDE specific log format
  if [service] == "bi-ide-api" {
    grok {
      match => {
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:message}",
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}"
        ]
      }
      overwrite => ["message"]
    }

    # Parse request logs
    if [message] =~ "HTTP" {
      grok {
        match => {
          "message" => "%{WORD:[http][request][method]} %{URIPATHPARAM:[http][request][path]} %{NUMBER:[http][response][status_code]:int} %{NUMBER:[event][duration]:int}ms"
        }
        add_field => {
          "[event][category]" => "web"
          "[event][type]" => "access"
        }
      }
    }

    # Parse AI request logs
    if [message] =~ "AI" or [message] =~ "OpenAI" {
      grok {
        match => {
          "message" => "AI request: model=%{DATA:[ai][model]}, tokens=%{NUMBER:[ai][tokens][total]:int}, cost=%{NUMBER:[ai][cost]:float}"
        }
        add_field => {
          "[event][category]" => "ai"
        }
      }
    }
  }

  # Parse Nginx access logs
  if [service] == "nginx" {
    grok {
      match => {
        "message" => '%{IPORHOST:[client][ip]} - %{DATA:[user][name]} \[%{HTTPDATE:timestamp}\] "%{WORD:[http][request][method]} %{URIPATHPARAM:[http][request][path]} HTTP/%{NUMBER:[http][version]}" %{NUMBER:[http][response][status_code]:int} %{NUMBER:[http][response][bytes]:int} "%{DATA:[http][request][referrer]}" "%{DATA:[user_agent][original]}" %{NUMBER:[event][duration]:int}'
      }
    }

    # Parse user agent
    useragent {
      source => "[user_agent][original]"
      target => "[user_agent]"
    }

    # Geolocation
    geoip {
      source => "[client][ip]"
      target => "[client][geo]"
      database => "/usr/share/logstash/GeoLite2-City.mmdb"
      default_database_type => "City"
      fields => ["city_name", "country_name", "country_iso_code", "location", "region_name"]
    }

    mutate {
      add_field => {
        "[@metadata][index_prefix]" => "nginx"
      }
    }
  }

  # Parse audit logs
  if [log_type] == "audit" {
    mutate {
      add_field => {
        "[@metadata][index_prefix]" => "audit"
        "[event][category]" => "authentication"
      }
    }
  }

  # Parse error logs
  if [log_level] == "error" or [level] == "ERROR" {
    mutate {
      add_field => {
        "[@metadata][index_prefix]" => "errors"
        "[event][category]" => "error"
        "[event][severity]" => "high"
      }
    }

    # Extract stack traces
    if [message] =~ "(?m)Exception|Error|Traceback" {
      grok {
        match => {
          "message" => "(?m)(?<[error][type]>[A-Za-z]+(?:Error|Exception)): (?<[error][message]>.*)"
        }
      }

      multiline {
        pattern => "^\\s+at|^\\s*Caused by:|^\\s*\\.{3} \\d+ more"
        what => "previous"
        stream_identity => "%{[host][name]}%{[file][path]}"
      }
    }
  }

  # Timestamp parsing
  date {
    match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS", "dd/MMM/yyyy:HH:mm:ss Z"]
    target => "@timestamp"
    remove_field => ["timestamp"]
  }

  # Set log level
  if ![log][level] {
    mutate {
      add_field => {
        "[log][level]" => "info"
      }
    }
  }

  # Convert log level to lowercase
  mutate {
    lowercase => ["[log][level]"]
  }

  # Add environment information
  if ![environment] {
    mutate {
      add_field => {
        "environment" => "production"
      }
    }
  }

  # Calculate response time buckets for percentile analysis
  if [event][duration] {
    ruby {
      code => "
        duration = event.get('[event][duration]')
        if duration
          if duration < 100
            event.set('[event][duration_bucket]', '0-100ms')
          elsif duration < 500
            event.set('[event][duration_bucket]', '100-500ms')
          elsif duration < 1000
            event.set('[event][duration_bucket]', '500ms-1s')
          elsif duration < 5000
            event.set('[event][duration_bucket]', '1-5s')
          else
            event.set('[event][duration_bucket]', '5s+')
          end
        end
      "
    }
  }

  # PII detection and masking
  if [message] {
    # Mask email addresses
    mutate {
      gsub => [
        "message", "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}", "[EMAIL_REDACTED]"
      ]
    }

    # Mask credit card numbers
    mutate {
      gsub => [
        "message", "\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b", "[CC_REDACTED]"
      ]
    }

    # Mask API keys
    mutate {
      gsub => [
        "message", "sk-[a-zA-Z0-9]{48}", "[API_KEY_REDACTED]"
      ]
    }
  }

  # Drop debug logs in production
  if [log][level] == "debug" and [environment] == "production" {
    drop { }
  }

  # Add tags based on conditions
  if [http][response][status_code] >= 500 {
    mutate {
      add_tag => ["error", "server_error"]
    }
  }

  if [event][duration] and [event][duration] > 5000 {
    mutate {
      add_tag => ["slow_request"]
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => [
      "[agent][ephemeral_id]",
      "[agent][hostname]",
      "[agent][id]",
      "[agent][name]",
      "[agent][type]",
      "[agent][version]",
      "[ecs][version]",
      "[input][type]",
      "[log][offset]",
      "[log][file][path]"
    ]
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["${ES_HOST:elasticsearch:9200}"]
    user => "${ES_USERNAME:logstash_writer}"
    password => "${ES_PASSWORD}"
    ssl => true
    ssl_certificate_verification => true
    cacert => "/etc/logstash/certs/ca.crt"

    # Index naming based on log type and date
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"

    # ILM settings
    ilm_enabled => true
    ilm_rollover_alias => "%{[@metadata][index_prefix]}"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "logstash-ilm-policy"

    # Performance tuning
    template_overwrite => false
    manage_template => false
    bulk_max_size => 500
    flush_size => 500
    idle_flush_time => 10
    retry_max_interval => 60
    retry_on_conflict => 3

    # Health check
    validate_after_inactivity => 10000
    sniffing => true
    sniffing_delay => 5
  }

  # Debug output (optional)
  if [environment] == "development" {
    stdout {
      codec => rubydebug
    }
  }

  # Archive to S3 (for long-term storage)
  if [event][category] == "audit" {
    s3 {
      region => "us-east-1"
      bucket => "bi-ide-v8-logs-archive"
      prefix => "audit/%{+YYYY/MM/dd}/"
      codec => "json_lines"
      canned_acl => "private"
      server_side_encryption => true
      time_file => 3600
      size_file => 104857600
    }
  }
}
