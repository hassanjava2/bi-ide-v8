"""
ğŸ§  Training Coordinator â€” Ù…Ù†Ø³Ù‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹

Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª:
- ØªÙˆØ²ÙŠØ¹ Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù‚Ø¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
- Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ø§Ù… ØªØ¯Ø±ÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù‡Ø±Ù…ÙŠØ©
- ØªØºØ°ÙŠØ© queue Ø¨Ù…Ù‡Ø§Ù… Ù…Ø³ØªÙ…Ø±Ø© (Ù„Ø§ idle)
- Ù…Ø±Ø§Ø¹Ø§Ø© Ù‚Ø¯Ø±Ø§Øª ÙƒÙ„ Ø¹Ù‚Ø¯Ø© (GPU vs CPU)
- Hostinger-aware scheduling (Ø®ÙÙŠÙ)
- ØªØ¹Ù„Ù… Ø°Ø§ØªÙŠ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
"""

from __future__ import annotations

import asyncio
import json
import os
import random
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

COORDINATOR_INTERVAL_SEC = int(os.getenv("COORDINATOR_INTERVAL_SEC", "30"))
MIN_QUEUE_SIZE = int(os.getenv("MIN_QUEUE_SIZE", "5"))
MAX_QUEUE_SIZE = int(os.getenv("MAX_QUEUE_SIZE", "50"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hierarchy Layers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TRAINING_LAYERS = [
    {
        "name": "council",
        "display": "Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ø£Ø¹Ù„Ù‰",
        "description": "Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©ØŒ ØªØ­Ù„ÙŠÙ„ØŒ ØªÙ†Ø¨Ø¤Ø§Øª",
        "requires_gpu": True,
        "priority": 9,
        "data_sources": ["data/knowledge/council_decisions.json"],
    },
    {
        "name": "scouts",
        "display": "Ø§Ù„ÙƒØ´Ø§ÙØ©",
        "description": "Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„ÙˆÙŠØ¨ØŒ Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙˆÙ‚",
        "requires_gpu": False,
        "priority": 7,
        "data_sources": ["data/knowledge/scout_reports.json"],
    },
    {
        "name": "erp_accounting",
        "display": "Ø§Ù„Ù…Ø­Ø§Ø³Ø¨Ø©",
        "description": "Ù…Ø­Ø§Ø³Ø¨Ø©ØŒ Ù…ÙŠØ²Ø§Ù†ÙŠØ§ØªØŒ ØªÙ‚Ø§Ø±ÙŠØ± Ù…Ø§Ù„ÙŠØ©",
        "requires_gpu": True,
        "priority": 8,
        "data_sources": ["data/learning/erp_data.json"],
    },
    {
        "name": "erp_inventory",
        "display": "Ø§Ù„Ù…Ø®Ø²ÙˆÙ†",
        "description": "Ø¥Ø¯Ø§Ø±Ø© Ù…Ø®Ø²ÙˆÙ†ØŒ ØªÙ†Ø¨ÙŠÙ‡ Ù†Ù‚ØµØŒ ØªØªØ¨Ø¹ Ø­Ø±ÙƒØ©",
        "requires_gpu": True,
        "priority": 8,
        "data_sources": ["data/learning/inventory_data.json"],
    },
    {
        "name": "code_generation",
        "display": "ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯",
        "description": "ÙƒØªØ§Ø¨Ø© ÙƒÙˆØ¯ØŒ Ø¥ØµÙ„Ø§Ø­ Ø£Ø®Ø·Ø§Ø¡ØŒ copilot",
        "requires_gpu": True,
        "priority": 9,
        "data_sources": ["data/learning/code_samples.json"],
    },
    {
        "name": "copilot",
        "display": "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ",
        "description": "Ø§ÙƒÙ…Ø§Ù„ Ø§Ù„ÙƒÙˆØ¯ØŒ Ø§Ù‚ØªØ±Ø§Ø­Ø§ØªØŒ ØªÙˆØ«ÙŠÙ‚",
        "requires_gpu": True,
        "priority": 8,
        "data_sources": ["data/learning/copilot_data.json"],
    },
    {
        "name": "balance",
        "display": "Ù…Ø¬Ù„Ø³ Ø§Ù„ØªÙˆØ§Ø²Ù†",
        "description": "Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ØŒ ØªÙˆØ§Ø²Ù† Ø§Ù„Ù…ÙˆØ§Ø±Ø¯",
        "requires_gpu": False,
        "priority": 6,
        "data_sources": ["data/learning/balance_logs.json"],
    },
    {
        "name": "meta_team",
        "display": "Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„ÙÙˆÙ‚ÙŠ",
        "description": "ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙØ±Ù‚ØŒ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª",
        "requires_gpu": False,
        "priority": 5,
        "data_sources": ["data/learning/meta_team_data.json"],
    },
    {
        "name": "domain_experts",
        "display": "Ø®Ø¨Ø±Ø§Ø¡ Ø§Ù„Ù…Ø¬Ø§Ù„",
        "description": "Ø®Ø¨Ø±Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª",
        "requires_gpu": True,
        "priority": 7,
        "data_sources": ["data/learning/expert_knowledge.json"],
    },
    {
        "name": "seventh_dimension",
        "display": "Ø§Ù„Ø¨Ø¹Ø¯ Ø§Ù„Ø³Ø§Ø¨Ø¹",
        "description": "Ø®Ø·Ø· Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰ØŒ Ø±Ø¤ÙŠØ© Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©",
        "requires_gpu": False,
        "priority": 4,
        "data_sources": ["data/learning/century_plan.json"],
    },
]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training Topics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TRAINING_TOPICS = {
    "council": [
        "ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©",
        "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ¥Ø¯Ø§Ø±ØªÙ‡Ø§",
        "ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ù…Ø§Ù„ÙŠØ©",
        "ØªØ­Ø³ÙŠÙ† consensus Ø¨ÙŠÙ† Ø§Ù„Ø­ÙƒÙ…Ø§Ø¡",
    ],
    "scouts": [
        "Ø§Ø³ØªÙƒØ´Ø§Ù Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø­Ø¯ÙŠØ«Ø©",
        "ØªØ­Ù„ÙŠÙ„ ØªÙ‡Ø¯ÙŠØ¯Ø§Øª Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ",
        "Ø±ØµØ¯ ÙØ±Øµ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©",
        "ØªØ­Ø³ÙŠÙ† ÙƒÙØ§Ø¡Ø© Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª",
    ],
    "erp_accounting": [
        "ØªØ­Ø³ÙŠÙ† ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø§Ø³Ø¨ÙŠØ©",
        "Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ Ø§Ù„Ù…Ø§Ù„ÙŠ",
        "ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø§Ù„Ø¢Ù„ÙŠØ©",
    ],
    "erp_inventory": [
        "ØªÙ†Ø¨Ø¤ Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª",
        "ØªØ­Ø³ÙŠÙ† Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø²ÙˆÙ† Ø§Ù„Ø°ÙƒÙŠØ©",
        "Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ù…ÙˆØ³Ù…ÙŠØ©",
    ],
    "code_generation": [
        "ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙˆÙ„Ø¯",
        "ØªØ¹Ù„Ù… Ø£Ù†Ù…Ø§Ø· Ø§Ù„ÙƒÙˆØ¯ Ù…Ù† Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹",
        "ØªØ­Ø³ÙŠÙ† Ø§ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ",
        "ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ø§Ù„Ù…Ø¹Ù‚Ø¯",
    ],
    "copilot": [
        "ØªØ­Ø³ÙŠÙ† Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ÙƒÙˆØ¯",
        "ØªØ¹Ù„Ù… Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬",
        "ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ø¢Ù„ÙŠ",
    ],
    "balance": [
        "ØªØ­Ø³ÙŠÙ† Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…",
        "Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø§Ø®ØªÙ†Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø¨ÙƒØ±Ø©",
    ],
    "meta_team": [
        "ØªØ­Ø³ÙŠÙ† ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙØ±Ù‚",
        "ØªØ­Ø³ÙŠÙ† ÙƒÙØ§Ø¡Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª",
    ],
    "domain_experts": [
        "ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ù…ØªØ®ØµØµØ©",
        "ØªÙˆØ³ÙŠØ¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©",
    ],
    "seventh_dimension": [
        "ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰",
        "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©",
    ],
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Coordinator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TrainingCoordinator:
    """
    Manages the training pipeline:
    1. Monitors queue size
    2. Auto-generates training tasks when queue is low
    3. Assigns tasks based on worker capabilities
    4. Tracks training history and improves scheduling
    """

    def __init__(self):
        self.is_running = False
        self.cycle_count = 0
        self.history_file = Path("data/learning/training_history.jsonl")
        self.history_file.parent.mkdir(parents=True, exist_ok=True)

    async def start(self, orchestrator_state):
        """Start the coordinator loop."""
        self.is_running = True
        self.state = orchestrator_state
        print("ğŸ§  Training Coordinator started")

        while self.is_running:
            try:
                await self._coordination_cycle()
                self.cycle_count += 1
            except Exception as e:
                print(f"âš ï¸ Coordinator error: {e}")

            await asyncio.sleep(COORDINATOR_INTERVAL_SEC)

    def stop(self):
        self.is_running = False
        print("ğŸ§  Training Coordinator stopped")

    async def _coordination_cycle(self):
        """One coordination cycle."""
        if not hasattr(self, 'state'):
            return

        # 1. Count queued jobs
        queued = [j for j in self.state.jobs.values() if j["status"] == "queued"]
        running = [j for j in self.state.jobs.values() if j["status"] == "running"]
        online_workers = [w for w in self.state.workers.values()
                         if w.get("status") in ("online", "idle", "training")]

        if not online_workers:
            return  # No workers, no point

        # 2. Fill queue if needed
        if len(queued) < MIN_QUEUE_SIZE:
            needed = MIN_QUEUE_SIZE - len(queued)
            new_tasks = self._generate_tasks(needed, online_workers)
            for task in new_tasks:
                self.state.jobs[task["job_id"]] = task

            if new_tasks:
                print(f"ğŸ§  Generated {len(new_tasks)} training tasks "
                      f"(queue: {len(queued)}â†’{len(queued)+len(new_tasks)}, "
                      f"running: {len(running)}, workers: {len(online_workers)})")

        # 3. Try to assign queued jobs to idle workers
        idle_workers = [w for w in online_workers
                       if not w.get("current_job") and w.get("status") != "throttled"]

        for worker in idle_workers:
            for job in queued:
                if job["status"] != "queued":
                    continue
                if self._worker_matches_job(worker, job):
                    job["status"] = "running"
                    job["assigned_worker"] = worker["worker_id"]
                    job["started_at"] = datetime.now(timezone.utc).isoformat()
                    worker["current_job"] = job["job_id"]
                    print(f"  ğŸ“‹ Assigned: {job['name']} â†’ {worker.get('hostname', worker['worker_id'])}")
                    break

    def _generate_tasks(self, count: int, workers: List[Dict]) -> List[Dict]:
        """Generate training tasks based on available workers."""
        tasks = []
        has_gpu = any(
            w.get("hardware", {}).get("gpu", {}).get("cuda_available", False)
            or w.get("hardware", {}).get("gpu", {}).get("name", "none") != "none"
            for w in workers
        )

        # Select layers based on available hardware
        available_layers = []
        for layer in TRAINING_LAYERS:
            if layer["requires_gpu"] and not has_gpu:
                continue
            available_layers.append(layer)

        if not available_layers:
            available_layers = [l for l in TRAINING_LAYERS if not l["requires_gpu"]]

        for _ in range(count):
            layer = random.choice(available_layers)
            topics = TRAINING_TOPICS.get(layer["name"], ["ØªØ¯Ø±ÙŠØ¨ Ø¹Ø§Ù…"])
            topic = random.choice(topics)

            import uuid
            job_id = str(uuid.uuid4())[:12]

            labels = ["gpu"] if layer["requires_gpu"] else ["cpu"]

            task = {
                "job_id": job_id,
                "name": f"{layer['display']}: {topic}",
                "command": "",  # Worker uses default training
                "shell": False,
                "target_labels": labels,
                "priority": layer["priority"],
                "config": {
                    "layer": layer["name"],
                    "topic": topic,
                    "data_sources": layer["data_sources"],
                },
                "layer_name": layer["name"],
                "auto_sync_to_primary": True,
                "status": "queued",
                "assigned_worker": None,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "started_at": None,
                "completed_at": None,
                "result": None,
                "artifacts": [],
                "logs": [],
            }
            tasks.append(task)

        return tasks

    def _worker_matches_job(self, worker: Dict, job: Dict) -> bool:
        """Check if a worker can run a specific job."""
        worker_labels = set(worker.get("labels", []))
        job_labels = set(job.get("target_labels", []))

        if not job_labels:
            return True  # No label requirement

        # Check label intersection
        if not worker_labels.intersection(job_labels):
            return False

        # Hostinger: skip GPU jobs
        if "hostinger" in worker_labels and "gpu" in job_labels:
            return False

        return True

    def _record_result(self, job: Dict):
        """Record training result for future scheduling optimization."""
        try:
            record = {
                "job_id": job["job_id"],
                "layer": job.get("layer_name"),
                "worker": job.get("assigned_worker"),
                "status": job["status"],
                "duration": None,
                "metrics": job.get("result"),
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
            if job.get("started_at") and job.get("completed_at"):
                # Calculate duration
                pass

            with open(self.history_file, "a") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        except Exception:
            pass

    def get_status(self) -> Dict[str, Any]:
        """Get coordinator status."""
        if not hasattr(self, 'state'):
            return {"is_running": self.is_running, "cycle_count": self.cycle_count}

        return {
            "is_running": self.is_running,
            "cycle_count": self.cycle_count,
            "queued_jobs": sum(1 for j in self.state.jobs.values() if j["status"] == "queued"),
            "running_jobs": sum(1 for j in self.state.jobs.values() if j["status"] == "running"),
            "completed_jobs": sum(1 for j in self.state.jobs.values() if j["status"] == "completed"),
            "total_workers": len(self.state.workers),
            "layers_count": len(TRAINING_LAYERS),
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Singleton â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

training_coordinator = TrainingCoordinator()
