"""
Evaluation Engine - Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
Merged from: evaluate-model.py, validate-data.py

Features / Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:
  â€¢ Model evaluation with perplexity calculation
  â€¢ Data validation and quality checks
  â€¢ Benchmark suite support
  â€¢ BLEU/ROUGE scores
  â€¢ Custom metrics
  â€¢ Report generation

PyTorch 2.x + CUDA 12.x Compatible
"""

import json
import os
import re
import hashlib
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Union, Any, Tuple, Callable
from dataclasses import dataclass, field
from collections import defaultdict
from enum import Enum

import numpy as np

logger = logging.getLogger(__name__)


class ValidationStatus(Enum):
    """Ø­Ø§Ù„Ø© Ø§Ù„ØªØ­Ù‚Ù‚ - Validation status"""
    VALID = "valid"
    TOO_SHORT = "too_short"
    TOO_LONG = "too_long"
    DUPLICATE = "duplicate"
    INVALID_FORMAT = "invalid_format"
    LOW_QUALITY = "low_quality"


class Language(Enum):
    """Ø§Ù„Ù„ØºØ© - Language"""
    ARABIC = "ar"
    ENGLISH = "en"
    MIXED = "mixed"
    UNKNOWN = "unknown"


@dataclass
class ValidationResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù‚Ù‚ - Validation result"""
    status: ValidationStatus
    sample: Dict[str, Any]
    message: str = ""
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DataQualityReport:
    """ØªÙ‚Ø±ÙŠØ± Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Data quality report"""
    timestamp: str
    total_samples: int
    valid_samples: int
    invalid_samples: int
    duplicates_removed: int
    by_language: Dict[str, int] = field(default_factory=dict)
    by_status: Dict[str, int] = field(default_factory=dict)
    samples: List[Dict] = field(default_factory=list)
    invalid_examples: List[Dict] = field(default_factory=list)
    
    @property
    def valid_ratio(self) -> float:
        """Ù†Ø³Ø¨Ø© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© - Valid ratio"""
        if self.total_samples == 0:
            return 0.0
        return self.valid_samples / self.total_samples


@dataclass
class EvaluationMetrics:
    """Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… - Evaluation metrics"""
    perplexity: Optional[float] = None
    bleu_score: Optional[float] = None
    rouge_l: Optional[float] = None
    accuracy: Optional[float] = None
    f1_score: Optional[float] = None
    custom_metrics: Dict[str, float] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            'perplexity': self.perplexity,
            'bleu_score': self.bleu_score,
            'rouge_l': self.rouge_l,
            'accuracy': self.accuracy,
            'f1_score': self.f1_score,
            **self.custom_metrics
        }


@dataclass
class ModelEvaluationReport:
    """ØªÙ‚Ø±ÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ - Model evaluation report"""
    timestamp: str
    model_path: str
    metrics: EvaluationMetrics
    samples_evaluated: int
    duration_seconds: float
    error: Optional[str] = None
    comparison_with_previous: Optional[Dict] = None


class EvaluationEngine:
    """
    Ù…Ø­Ø±Ùƒ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… - Evaluation Engine
    
    ÙŠÙˆÙØ± ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„Ø©:
    - ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (perplexity, BLEU, ROUGE)
    - Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    - Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØªÙƒØ±Ø§Ø±
    - ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
    
    Provides comprehensive evaluation:
    - Model evaluation (perplexity, BLEU, ROUGE)
    - Data quality validation
    - Duplicate detection
    - Report generation
    """
    
    # Ø§Ù„Ø«ÙˆØ§Ø¨Øª - Constants
    MIN_LENGTH = 10
    MAX_LENGTH = 8000
    ARABIC_PATTERN = re.compile(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF]+')
    
    def __init__(
        self,
        base_dir: Optional[Path] = None,
        output_dir: Optional[Path] = None
    ):
        """
        Initialize evaluation engine
        
        Args:
            base_dir: Base project directory
            output_dir: Output directory for reports
        """
        self.base_dir = base_dir or Path(__file__).parent.parent.parent
        
        if output_dir is None:
            self.output_dir = self.base_dir / "training" / "output"
        else:
            self.output_dir = output_dir
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ” EvaluationEngine initialized")
        logger.info(f"   Output: {self.output_dir}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Data Validation Functions
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def count_arabic(self, text: str) -> int:
        """Ø¹Ø¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - Count Arabic characters"""
        return len(self.ARABIC_PATTERN.findall(text)) if text else 0
    
    def count_english(self, text: str) -> int:
        """Ø¹Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© - Count English words"""
        return len(re.findall(r'[a-zA-Z]+', text or ''))
    
    def detect_language(self, text: str) -> Language:
        """Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù„ØºØ© - Detect language"""
        arabic_count = self.count_arabic(text)
        english_count = self.count_english(text)
        
        if arabic_count > english_count:
            return Language.ARABIC
        elif english_count > arabic_count:
            return Language.ENGLISH
        elif arabic_count > 0 and english_count > 0:
            return Language.MIXED
        return Language.UNKNOWN
    
    def get_sample_signature(self, sample: Dict[str, Any]) -> str:
        """
        ØªÙˆÙ„ÙŠØ¯ ØªÙˆÙ‚ÙŠØ¹ Ù„Ù„Ø¹ÙŠÙ†Ø© (Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±)
        Generate sample signature (for duplicate detection)
        
        Args:
            sample: Data sample
            
        Returns:
            Hash signature
        """
        if isinstance(sample, dict):
            inp = sample.get('input', sample.get('prompt', sample.get('question', sample.get('instruction', ''))))
            out = sample.get('output', sample.get('response', sample.get('completion', sample.get('answer', ''))))
            key = f"{inp}|{out}"
        else:
            key = str(sample)
        return hashlib.sha256(key.encode('utf-8')).hexdigest()[:16]
    
    def get_text_length(self, sample: Dict[str, Any]) -> int:
        """Ø­Ø³Ø§Ø¨ Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ - Calculate text length"""
        d = sample.get('data', sample) if isinstance(sample, dict) and 'data' in sample else sample
        
        if not isinstance(d, dict):
            return len(str(d))
        
        inp = d.get('input', d.get('prompt', d.get('question', d.get('instruction', '')))) or ''
        out = d.get('output', d.get('response', d.get('completion', d.get('answer', '')))) or ''
        return len(inp) + len(out)
    
    def validate_sample(
        self,
        sample: Dict[str, Any],
        seen_signatures: Optional[set] = None
    ) -> ValidationResult:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹ÙŠÙ†Ø© ÙˆØ§Ø­Ø¯Ø©
        Validate a single sample
        
        Args:
            sample: Data sample
            seen_signatures: Set of seen signatures for duplicate detection
            
        Returns:
            Validation result
        """
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±
        if seen_signatures is not None:
            sig = self.get_sample_signature(sample)
            if sig in seen_signatures:
                return ValidationResult(
                    status=ValidationStatus.DUPLICATE,
                    sample=sample,
                    message="Duplicate sample detected"
                )
            seen_signatures.add(sig)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„
        length = self.get_text_length(sample)
        if length < self.MIN_LENGTH:
            return ValidationResult(
                status=ValidationStatus.TOO_SHORT,
                sample=sample,
                message=f"Sample too short ({length} < {self.MIN_LENGTH})",
                details={'length': length}
            )
        
        if length > self.MAX_LENGTH:
            return ValidationResult(
                status=ValidationStatus.TOO_LONG,
                sample=sample,
                message=f"Sample too long ({length} > {self.MAX_LENGTH})",
                details={'length': length}
            )
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        d = sample.get('data', sample) if isinstance(sample, dict) and 'data' in sample else sample
        if isinstance(d, dict):
            inp = d.get('input', d.get('prompt', d.get('question', d.get('instruction', ''))))
            out = d.get('output', d.get('response', d.get('completion', d.get('answer', ''))))
            
            if not inp or not out:
                return ValidationResult(
                    status=ValidationStatus.INVALID_FORMAT,
                    sample=sample,
                    message="Missing input or output"
                )
        
        return ValidationResult(
            status=ValidationStatus.VALID,
            sample=sample,
            message="Valid sample"
        )
    
    def validate_data(
        self,
        data: List[Dict[str, Any]],
        remove_duplicates: bool = True,
        fix: bool = False
    ) -> DataQualityReport:
        """
        Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª
        Validate a dataset
        
        Args:
            data: List of data samples
            remove_duplicates: Whether to remove duplicates
            fix: Whether to fix issues automatically
            
        Returns:
            Data quality report
        """
        logger.info(f"ğŸ” Validating {len(data)} samples...")
        
        seen = set() if remove_duplicates else None
        results = []
        valid_list = []
        by_language = defaultdict(int)
        by_status = defaultdict(int)
        
        for sample in data:
            result = self.validate_sample(sample, seen)
            results.append(result)
            by_status[result.status.value] += 1
            
            if result.status == ValidationStatus.VALID:
                valid_list.append(sample)
                # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ©
                d = sample.get('data', sample) if isinstance(sample, dict) and 'data' in sample else sample
                if isinstance(d, dict):
                    inp = d.get('input', d.get('prompt', d.get('question', d.get('instruction', '')))) or ''
                    lang = self.detect_language(str(inp))
                    by_language[lang.value] += 1
        
        duplicates = by_status.get(ValidationStatus.DUPLICATE.value, 0)
        
        report = DataQualityReport(
            timestamp=datetime.now().isoformat(),
            total_samples=len(data),
            valid_samples=len(valid_list),
            invalid_samples=len(data) - len(valid_list) - duplicates,
            duplicates_removed=duplicates,
            by_language=dict(by_language),
            by_status=dict(by_status),
            samples=valid_list if fix else [],
            invalid_examples=[
                {'status': r.status.value, 'message': r.message, 'details': r.details}
                for r in results if r.status != ValidationStatus.VALID
            ][:100]
        )
        
        logger.info(f"   Total: {report.total_samples}")
        logger.info(f"   Valid: {report.valid_samples}")
        logger.info(f"   Duplicates: {report.duplicates_removed}")
        logger.info(f"   Valid ratio: {report.valid_ratio:.1%}")
        
        return report
    
    def load_and_validate_directory(
        self,
        directory: Optional[Path] = None,
        pattern: str = "*.json",
        fix: bool = False
    ) -> DataQualityReport:
        """
        ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø¬Ù„Ø¯
        Load and validate directory
        
        Args:
            directory: Directory to load from
            pattern: File pattern
            fix: Whether to save fixed data
            
        Returns:
            Data quality report
        """
        if directory is None:
            directory = self.base_dir / "training" / "output"
        
        all_samples = []
        
        for json_file in directory.rglob(pattern):
            if json_file.name.startswith('.'):
                continue
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if isinstance(data, list):
                    for i, item in enumerate(data):
                        all_samples.append({
                            'source': str(json_file),
                            'index': i,
                            'data': item
                        })
                elif isinstance(data, dict):
                    if 'samples' in data:
                        for i, item in enumerate(data['samples']):
                            all_samples.append({
                                'source': str(json_file),
                                'index': i,
                                'data': item
                            })
            except Exception as e:
                logger.warning(f"   âš ï¸ {json_file}: {e}")
        
        logger.info(f"ğŸ“‚ Loaded {len(all_samples)} raw samples from {directory}")
        
        report = self.validate_data(all_samples, fix=fix)
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙØµØ­Ø­Ø©
        if fix and report.samples:
            output_path = self.output_dir / "validated_training_data.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report.samples, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ Saved validated data: {output_path}")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_path = self.output_dir / "validation_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': report.timestamp,
                'total': report.total_samples,
                'valid': report.valid_samples,
                'duplicates_removed': report.duplicates_removed,
                'by_language': report.by_language,
                'by_status': report.by_status,
                'valid_ratio': report.valid_ratio
            }, f, ensure_ascii=False, indent=2)
        
        return report
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ÙˆØ¸Ø§Ø¦Ù ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ - Model Evaluation Functions
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def compute_perplexity(
        self,
        model_path: Union[str, Path],
        samples: List[Dict[str, Any]],
        max_samples: int = 50,
        max_length: int = 512
    ) -> Tuple[Optional[float], Optional[str]]:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ perplexity
        Compute perplexity
        
        Args:
            model_path: Path to model
            samples: Validation samples
            max_samples: Maximum samples to evaluate
            max_length: Maximum sequence length
            
        Returns:
            (perplexity, error_message)
        """
        try:
            import torch
            from transformers import AutoModelForCausalLM, AutoTokenizer
        except ImportError as e:
            return None, f"Missing dependencies: {e}"
        
        try:
            logger.info(f"ğŸ¤– Loading model: {model_path}")
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float32,
                trust_remote_code=True
            )
            model.eval()
            
            total_loss = 0.0
            count = 0
            
            for sample in samples[:max_samples]:
                text = self._get_text_from_sample(sample)
                if len(text) < 20:
                    continue
                
                inputs = tokenizer(
                    text[:max_length],
                    return_tensors='pt',
                    truncation=True
                )
                
                with torch.no_grad():
                    outputs = model(
                        **inputs,
                        labels=inputs['input_ids']
                    )
                    total_loss += outputs.loss.item()
                    count += 1
            
            if count == 0:
                return None, "No valid samples"
            
            avg_loss = total_loss / count
            perplexity = float(torch.exp(torch.tensor(avg_loss)).item())
            
            logger.info(f"   Perplexity: {perplexity:.2f} (evaluated on {count} samples)")
            return perplexity, None
            
        except Exception as e:
            return None, str(e)
    
    def _get_text_from_sample(self, sample: Any) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø¹ÙŠÙ†Ø© - Extract text from sample"""
        if isinstance(sample, dict):
            # Handle different formats
            inp = sample.get('input', sample.get('prompt', sample.get('question', sample.get('instruction', '')))) or ''
            out = sample.get('output', sample.get('response', sample.get('completion', sample.get('answer', '')))) or ''
            
            # Handle nested 'data' key
            if not inp and not out and 'data' in sample:
                return self._get_text_from_sample(sample['data'])
            
            return str(inp) + '\n' + str(out)
        return str(sample)
    
    def compute_bleu(
        self,
        predictions: List[str],
        references: List[str]
    ) -> Optional[float]:
        """
        Ø­Ø³Ø§Ø¨ BLEU score
        Compute BLEU score
        
        Args:
            predictions: Predicted texts
            references: Reference texts
            
        Returns:
            BLEU score
        """
        try:
            from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
            
            scores = []
            smooth = SmoothingFunction()
            
            for pred, ref in zip(predictions, references):
                pred_tokens = pred.split()
                ref_tokens = ref.split()
                
                score = sentence_bleu(
                    [ref_tokens],
                    pred_tokens,
                    smoothing_function=smooth.method1
                )
                scores.append(score)
            
            return float(np.mean(scores)) if scores else None
            
        except ImportError:
            logger.warning("nltk not installed, skipping BLEU")
            return None
        except Exception as e:
            logger.warning(f"BLEU calculation failed: {e}")
            return None
    
    def compute_rouge(
        self,
        predictions: List[str],
        references: List[str]
    ) -> Optional[Dict[str, float]]:
        """
        Ø­Ø³Ø§Ø¨ ROUGE scores
        Compute ROUGE scores
        
        Args:
            predictions: Predicted texts
            references: Reference texts
            
        Returns:
            Dict with ROUGE scores
        """
        try:
            from rouge_score import rouge_scorer
            
            scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
            
            scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}
            
            for pred, ref in zip(predictions, references):
                score = scorer.score(ref, pred)
                for key in scores:
                    scores[key].append(score[key].fmeasure)
            
            return {
                key: float(np.mean(values))
                for key, values in scores.items()
            }
            
        except ImportError:
            logger.warning("rouge_score not installed, skipping ROUGE")
            return None
        except Exception as e:
            logger.warning(f"ROUGE calculation failed: {e}")
            return None
    
    def evaluate_model(
        self,
        model_path: Union[str, Path],
        validation_data: Optional[List[Dict]] = None,
        compute_bleu_rouge: bool = False
    ) -> ModelEvaluationReport:
        """
        ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„
        Comprehensive model evaluation
        
        Args:
            model_path: Path to model
            validation_data: Validation samples
            compute_bleu_rouge: Whether to compute BLEU/ROUGE
            
        Returns:
            Evaluation report
        """
        start_time = time.time()
        model_path = Path(model_path)
        
        logger.info("=" * 50)
        logger.info("Model Evaluation - ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
        logger.info("=" * 50)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        if not model_path.exists():
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            candidates = [
                self.base_dir / "models" / "merged",
                self.base_dir / "models" / "finetuned-extended",
                self.base_dir / "models" / "finetuned",
            ]
            for candidate in candidates:
                if candidate.exists():
                    model_path = candidate
                    break
        
        if not model_path.exists():
            return ModelEvaluationReport(
                timestamp=datetime.now().isoformat(),
                model_path=str(model_path),
                metrics=EvaluationMetrics(),
                samples_evaluated=0,
                duration_seconds=0,
                error=f"Model not found: {model_path}"
            )
        
        # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚
        if validation_data is None:
            validation_path = self.output_dir / "validated_training_data.json"
            if validation_path.exists():
                with open(validation_path, 'r', encoding='utf-8') as f:
                    validation_data = json.load(f)
            else:
                all_data_path = self.output_dir / "all_training_data.json"
                if all_data_path.exists():
                    with open(all_data_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        validation_data = data if isinstance(data, list) else data.get('samples', [])
        
        validation_data = validation_data or []
        
        # Ø­Ø³Ø§Ø¨ Perplexity
        perplexity, error = self.compute_perplexity(model_path, validation_data)
        
        metrics = EvaluationMetrics(perplexity=perplexity)
        
        # Ø­Ø³Ø§Ø¨ BLEU/ROUGE Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨
        if compute_bleu_rouge and validation_data:
            # Ù‡Ø°Ø§ ÙŠØªØ·Ù„Ø¨ ØªÙˆÙ„ÙŠØ¯ ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            # ÙŠÙ…ÙƒÙ† ØªÙˆØ³ÙŠØ¹Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
            pass
        
        duration = time.time() - start_time
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        prev_report = None
        prev_report_path = self.output_dir / "evaluation_report_previous.json"
        current_report_path = self.output_dir / "evaluation_report.json"
        
        if current_report_path.exists():
            try:
                with open(current_report_path, 'r', encoding='utf-8') as f:
                    prev_report = json.load(f)
                shutil.copy2(current_report_path, prev_report_path)
            except:
                pass
        
        # Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        comparison = None
        if prev_report and prev_report.get('metrics', {}).get('perplexity'):
            prev_ppl = prev_report['metrics']['perplexity']
            if perplexity:
                diff = perplexity - prev_ppl
                comparison = {
                    'previous_perplexity': prev_ppl,
                    'current_perplexity': perplexity,
                    'difference': diff,
                    'improved': diff < 0
                }
                logger.info(f"   Comparison: {prev_ppl:.2f} â†’ {perplexity:.2f} ({diff:+.2f})")
        
        report = ModelEvaluationReport(
            timestamp=datetime.now().isoformat(),
            model_path=str(model_path),
            metrics=metrics,
            samples_evaluated=len(validation_data),
            duration_seconds=duration,
            error=error,
            comparison_with_previous=comparison
        )
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        with open(current_report_path, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': report.timestamp,
                'model_path': report.model_path,
                'metrics': metrics.to_dict(),
                'samples_evaluated': report.samples_evaluated,
                'duration_seconds': report.duration_seconds,
                'error': report.error,
                'comparison': report.comparison_with_previous
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nâœ… Evaluation complete!")
        logger.info(f"   Report: {current_report_path}")
        
        return report
    
    def run_benchmark_suite(
        self,
        model_path: Union[str, Path],
        benchmarks: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ØªØ´ØºÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        Run benchmark suite
        
        Args:
            model_path: Path to model
            benchmarks: List of benchmark names
            
        Returns:
            Benchmark results
        """
        benchmarks = benchmarks or ['perplexity', 'validation']
        results = {}
        
        logger.info("=" * 50)
        logger.info("Benchmark Suite - Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª")
        logger.info("=" * 50)
        
        if 'validation' in benchmarks:
            report = self.load_and_validate_directory(fix=True)
            results['validation'] = {
                'valid_ratio': report.valid_ratio,
                'total_samples': report.total_samples,
                'valid_samples': report.valid_samples
            }
        
        if 'perplexity' in benchmarks:
            eval_report = self.evaluate_model(model_path)
            results['perplexity'] = {
                'score': eval_report.metrics.perplexity,
                'samples': eval_report.samples_evaluated
            }
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results_path = self.output_dir / "benchmark_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'model_path': str(model_path),
                'results': results
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ’¾ Benchmark results: {results_path}")
        
        return results


# Import shutil for file operations
import shutil


if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø±
    print("=" * 50)
    print("Evaluation Engine - Test")
    print("=" * 50)
    
    engine = EvaluationEngine()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ­Ù‚Ù‚
    test_data = [
        {'instruction': 'What is Python?', 'output': 'A programming language'},
        {'instruction': 'Hi', 'output': 'Hello!'},
        {'instruction': 'A' * 10000, 'output': 'Too long'},
    ]
    
    report = engine.validate_data(test_data)
    print(f"\nValidation: {report.valid_ratio:.1%} valid")
    
    print("\nâœ… EvaluationEngine ready!")
