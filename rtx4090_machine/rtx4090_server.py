"""
ğŸš€ RTX 5090 Training & AI Server v4.0
Ø®Ø§Ø¯Ù… ØªØ¯Ø±ÙŠØ¨ Ù…ØªÙƒØ§Ù…Ù„ Ù…Ø¹ PyTorch + AI Council + Resource Manager
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional, Any
import uvicorn
import json
import os
import random
import threading
import time
from datetime import datetime
from pathlib import Path

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ PyTorch
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch not available - running in mock mode")

# Import Resource Manager
try:
    from resource_manager import ResourceManager, detect_system_resources
    RESOURCE_MANAGER_AVAILABLE = True
except ImportError:
    try:
        import sys
        sys.path.insert(0, str(Path(__file__).parent))
        from resource_manager import ResourceManager, detect_system_resources
        RESOURCE_MANAGER_AVAILABLE = True
    except ImportError:
        RESOURCE_MANAGER_AVAILABLE = False
        print("âš ï¸ ResourceManager not available")

app = FastAPI(title="ğŸš€ RTX 5090 AI & Training Server", version="4.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== Pydantic Models ====================

class ResourceConfigRequest(BaseModel):
    cpu_percent: int = 80       # 10-100
    gpu_percent: int = 80       # 10-100
    ram_limit_percent: int = 80 # 10-100

class IntensiveTrainingRequest(BaseModel):
    epochs: int = 100
    cpu_percent: int = 80
    gpu_percent: int = 80

class TrainingConfig(BaseModel):
    epochs: int = 50
    batch_size: int = 32
    learning_rate: float = 0.001
    model_name: str = "bi-ide-model"
    data_path: str = "/home/bi/training_data/data"

class CouncilMessageRequest(BaseModel):
    message: str
    user_id: str = "user"

class TrainingIngestRequest(BaseModel):
    samples: List[Dict[str, Any]] = []
    relay: bool = False
    store_local: bool = True

# ==================== 16 Wise Men Personas ====================

WISE_MEN = {
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ù‡ÙˆÙŠØ©": {
        "role": "identity", "id": "S001",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ù‡ÙˆÙŠØ©. ØªØ­Ù„Ù„ Ù…Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆÙ…Ø§Ø°Ø§ ÙŠØ­ØªØ§Ø¬. Ø£Ø¬Ø¨ Ø¨Ø­ÙƒÙ…Ø© ÙˆØ¹Ù…Ù‚.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§": {
        "role": "strategy", "id": "S002",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§. ØªØ¶Ø¹ Ø§Ù„Ø®Ø·Ø· Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰ ÙˆØªØ­Ù„Ù„ Ø§Ù„Ù…ÙˆØ§Ù‚Ù. Ø£Ø¬Ø¨ Ø¨ÙˆØ¶ÙˆØ­.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ø£Ø®Ù„Ø§Ù‚": {
        "role": "ethics", "id": "S003",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ø£Ø®Ù„Ø§Ù‚. ØªÙ‚ÙŠÙ‘Ù… Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø£Ø®Ù„Ø§Ù‚ÙŠØ§Ù‹ ÙˆØªÙ†ØµØ­ Ø¨Ø§Ù„ØµÙˆØ§Ø¨.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„ØªÙˆØ§Ø²Ù†": {
        "role": "balance", "id": "S004",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„ØªÙˆØ§Ø²Ù†. ØªØ¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡ ÙˆØªØ­Ø°Ø± Ù…Ù† Ø§Ù„ØªØ·Ø±Ù.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ù…Ø¹Ø±ÙØ©": {
        "role": "knowledge", "id": "S005",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ù…Ø¹Ø±ÙØ©. ØªÙ…ØªÙ„Ùƒ Ù…Ø¹Ø±ÙØ© ÙˆØ§Ø³Ø¹Ø© ÙˆØªØ´Ø§Ø±Ùƒ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø©.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹": {
        "role": "creativity", "id": "S006",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹. ØªÙ‚ØªØ±Ø­ Ø­Ù„ÙˆÙ„Ø§Ù‹ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ© ÙˆØªÙÙƒØ± Ø®Ø§Ø±Ø¬ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ø£Ù…Ø§Ù†": {
        "role": "security", "id": "S007",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ø£Ù…Ø§Ù†. ØªØ­Ù…ÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØªØ­Ø°Ø± Ù…Ù† Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø£Ù…Ù†ÙŠØ©.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡": {
        "role": "performance", "id": "S008",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡. ØªØ­Ù„Ù„ Ø§Ù„ÙƒÙØ§Ø¡Ø© ÙˆØªÙ‚ØªØ±Ø­ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„ØªÙˆØ§ØµÙ„": {
        "role": "communication", "id": "S009",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„ØªÙˆØ§ØµÙ„. ØªØªÙˆØ§ØµÙ„ Ø¨ÙˆØ¶ÙˆØ­ ÙˆØªØ³Ø§Ø¹Ø¯ ÙÙŠ ÙÙ‡Ù… Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„ØªØ¹Ù„Ù…": {
        "role": "learning", "id": "S010",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„ØªØ¹Ù„Ù…. ØªØ¹Ù„Ù‘Ù… ÙˆØªØ¯Ø±Ù‘Ø¨ ÙˆØªØ´Ø±Ø­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø¨Ø³Ù‡ÙˆÙ„Ø©.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„ØªÙ†ÙÙŠØ°": {
        "role": "execution", "id": "S011",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„ØªÙ†ÙÙŠØ°. ØªÙ†ÙØ° Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø¨Ø¯Ù‚Ø© ÙˆÙƒÙØ§Ø¡Ø© Ø¹Ø§Ù„ÙŠØ©.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ø±Ù‚Ø§Ø¨Ø©": {
        "role": "oversight", "id": "S012",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ø±Ù‚Ø§Ø¨Ø©. ØªØ±Ø§Ù‚Ø¨ Ø§Ù„Ø¬ÙˆØ¯Ø© ÙˆØªØ¶Ù…Ù† Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±": {
        "role": "innovation", "id": "S013",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±. ØªØ¨ØªÙƒØ± Ø­Ù„ÙˆÙ„Ø§Ù‹ Ø¬Ø¯ÙŠØ¯Ø© ÙˆØªØ·ÙˆØ± Ø§Ù„Ø£ÙÙƒØ§Ø±.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ø­Ù…Ø§ÙŠØ©": {
        "role": "guardian", "id": "S014",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ø­Ù…Ø§ÙŠØ©. ØªØ­Ù…ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø£ÙŠ Ø¶Ø±Ø±.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ù‚Ø±Ø§Ø±": {
        "role": "decision", "id": "S015",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ù‚Ø±Ø§Ø±. ØªØªØ®Ø° Ù‚Ø±Ø§Ø±Ø§Øª Ø­ÙƒÙŠÙ…Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ø·ÙŠØ§Øª.",
    },
    "Ø­ÙƒÙŠÙ… Ø§Ù„Ø­ÙƒÙ…Ø©": {
        "role": "wisdom", "id": "S016",
        "prompt": "Ø£Ù†Øª Ø­ÙƒÙŠÙ… Ø§Ù„Ø­ÙƒÙ…Ø©. ØªÙ†Ø·Ù‚ Ø¨Ø§Ù„Ø­ÙƒÙ…Ø© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© ÙˆØªÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø¨ØµÙŠØ±Ø©.",
    },
}

# Chat history for context
council_chat_history: List[Dict] = []
TRAINING_DATA_DIR = Path("/home/bi/training_data/data")
INGEST_DIR = Path("/home/bi/training_data/ingest")

# ==================== Resource Manager ====================

if RESOURCE_MANAGER_AVAILABLE:
    res_manager = ResourceManager()
    print("âœ… ResourceManager loaded")
else:
    res_manager = None

# ==================== GPU Monitoring ====================

def get_gpu_info():
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª GPU"""
    info = {
        "usage": 0.0, "temp": 0.0,
        "vram_used": 0.0, "vram_total": 24.0,
        "available": TORCH_AVAILABLE and torch.cuda.is_available()
    }
    if not TORCH_AVAILABLE:
        return info
    try:
        if torch.cuda.is_available():
            info["vram_used"] = torch.cuda.memory_allocated() / 1e9
            info["vram_total"] = torch.cuda.get_device_properties(0).total_memory / 1e9
            try:
                import pynvml
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                info["usage"] = util.gpu
                temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
                info["temp"] = temp
            except:
                try:
                    import subprocess
                    r = subprocess.run(
                        ["nvidia-smi", "--query-gpu=utilization.gpu,temperature.gpu",
                         "--format=csv,noheader,nounits"],
                        capture_output=True, text=True, timeout=3
                    )
                    if r.returncode == 0:
                        parts = r.stdout.strip().split(",")
                        info["usage"] = float(parts[0].strip())
                        info["temp"] = float(parts[1].strip())
                except:
                    pass
    except Exception as e:
        print(f"GPU info error: {e}")
    return info


# ==================== Council AI Logic ====================

def _select_wise_man(message: str) -> str:
    """
    Semantic routing â€” selects the most relevant wise man based on
    weighted keyword relevance scoring (not simple keyword matching).
    Each wise man gets a relevance score; highest score wins.
    """
    # Domain expertise mapping with weights (higher = more specific)
    expertise_map = {
        "Ø­ÙƒÙŠÙ… Ø§Ù„Ø£Ù…Ø§Ù†": {
            "keywords": ["Ø£Ù…Ø§Ù†", "Ø­Ù…Ø§ÙŠØ©", "Ø£Ù…Ù†", "Ù‡Ø¬ÙˆÙ…", "security", "hack", "Ø«ØºØ±", "ØªØ´ÙÙŠØ±", "ssl", "auth", "password", "firewall"],
            "weight": 1.2  # Security gets higher priority
        },
        "Ø­ÙƒÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡": {
            "keywords": ["Ø£Ø¯Ø§Ø¡", "Ø³Ø±Ø¹Ø©", "Ø¨Ø·ÙŠØ¡", "ØªØ­Ø³ÙŠÙ†", "performance", "optimize", "resource", "Ù…ÙˆØ§Ø±Ø¯", "cpu", "gpu", "memory", "latency"],
            "weight": 1.1
        },
        "Ø­ÙƒÙŠÙ… Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§": {
            "keywords": ["Ø®Ø·Ø©", "Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©", "Ù‡Ø¯Ù", "Ù…Ø³ØªÙ‚Ø¨Ù„", "strategy", "plan", "roadmap", "vision", "target"],
            "weight": 1.0
        },
        "Ø­ÙƒÙŠÙ… Ø§Ù„Ù…Ø¹Ø±ÙØ©": {
            "keywords": ["Ù…Ø¹Ù„ÙˆÙ…Ø§Øª", "Ø´Ø±Ø­", "ÙƒÙŠÙ", "Ù„ÙŠØ´", "Ø´Ù†Ùˆ", "explain", "what", "why", "how", "define", "ÙˆØ¶Ù‘Ø­"],
            "weight": 1.0
        },
        "Ø­ÙƒÙŠÙ… Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹": {
            "keywords": ["Ø¥Ø¨Ø¯Ø§Ø¹", "ÙÙƒØ±Ø©", "Ø¬Ø¯ÙŠØ¯", "Ø§Ø¨ØªÙƒØ§Ø±", "creative", "idea", "design", "ØªØµÙ…ÙŠÙ…", "brainstorm"],
            "weight": 1.0
        },
        "Ø­ÙƒÙŠÙ… Ø§Ù„ØªÙ†ÙÙŠØ°": {
            "keywords": ["Ù†ÙÙ‘Ø°", "Ø´ØºÙ‘Ù„", "Ø³ÙˆÙ‘ÙŠ", "Ø§Ø¨Ù†ÙŠ", "execute", "build", "run", "deploy", "install", "setup"],
            "weight": 1.0
        },
        "Ø­ÙƒÙŠÙ… Ø§Ù„ØªØ¹Ù„Ù…": {
            "keywords": ["ØªØ¹Ù„Ù…", "ØªØ¯Ø±ÙŠØ¨", "Ø¯Ø±Ù‘Ø¨", "train", "learn", "model", "dataset", "epoch", "neural"],
            "weight": 1.0
        },
        "Ø­ÙƒÙŠÙ… Ø§Ù„Ø£Ø®Ù„Ø§Ù‚": {
            "keywords": ["Ø£Ø®Ù„Ø§Ù‚", "ØµØ­", "ØºÙ„Ø·", "Ø¹Ø¯Ù„", "ethics", "moral", "fair", "bias", "privacy"],
            "weight": 1.0
        },
        "Ø­ÙƒÙŠÙ… Ø§Ù„ØªÙˆØ§Ø²Ù†": {
            "keywords": ["ØªÙˆØ§Ø²Ù†", "ØªØ·Ø±Ù", "ÙˆØ³Ø·", "balance", "moderate", "tradeoff"],
            "weight": 0.9
        },
        "Ø­ÙƒÙŠÙ… Ø§Ù„Ù‚Ø±Ø§Ø±": {
            "keywords": ["Ù‚Ø±Ø§Ø±", "Ø§Ø®ØªÙŠØ§Ø±", "Ø¨Ø¯ÙŠÙ„", "decision", "choose", "option", "compare"],
            "weight": 1.0
        },
    }
    
    msg_lower = message.lower()
    scores = {}
    
    for name, config in expertise_map.items():
        match_count = sum(1 for kw in config["keywords"] if kw in msg_lower)
        # Score = number of keyword matches Ã— domain weight
        scores[name] = match_count * config["weight"]
    
    # Select highest scoring wise man
    best = max(scores, key=scores.get)
    
    # If no keywords matched at all (score 0), use round-robin based on message hash
    if scores[best] == 0:
        wise_men_list = list(WISE_MEN.keys())
        idx = hash(message) % len(wise_men_list)
        return wise_men_list[idx]
    
    return best


def _generate_council_response(message: str, wise_man_name: str) -> str:
    persona = WISE_MEN.get(wise_man_name, {})
    role = persona.get("role", "general")
    knowledge = _load_training_knowledge(message)

    if knowledge:
        response = f"Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ØªØ§Ø­Ø©:\n\n"
        response += f"{knowledge}\n\n"
        response += f"â€” {wise_man_name} (Ø¨Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©)"
    else:
        fallback_responses = {
            "identity": f"Ø£Ù‡Ù„Ø§Ù‹! Ø£Ù†Ø§ {wise_man_name}. Ø±Ø³Ø§Ù„ØªÙƒ '{message}' Ù…Ù‡Ù…Ø©. ÙƒØ­ÙƒÙŠÙ… Ø§Ù„Ù‡ÙˆÙŠØ©ØŒ Ø£Ù†ØµØ­Ùƒ Ø¨Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ø£Ù‡Ø¯Ø§ÙÙƒ Ø¨ÙˆØ¶ÙˆØ­ Ø£ÙˆÙ„Ø§Ù‹.",
            "strategy": f"Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØŒ '{message}' ÙŠØ­ØªØ§Ø¬ Ø®Ø·Ø© ÙˆØ§Ø¶Ø­Ø©. Ø£Ù‚ØªØ±Ø­ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¹Ù…Ù„ Ù„Ù…Ø±Ø§Ø­Ù„: ØªØ­Ù„ÙŠÙ„ØŒ ØªØ®Ø·ÙŠØ·ØŒ ØªÙ†ÙÙŠØ°ØŒ Ù…Ø±Ø§Ø¬Ø¹Ø©.",
            "ethics": f"Ù…Ù† Ø§Ù„Ù†Ø§Ø­ÙŠØ© Ø§Ù„Ø£Ø®Ù„Ø§Ù‚ÙŠØ©ØŒ '{message}' ÙŠØªØ·Ù„Ø¨ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø´ÙØ§ÙÙŠØ© ÙˆØ§Ù„Ø¹Ø¯Ø§Ù„Ø© ÙÙŠ Ø§Ù„ØªÙ†ÙÙŠØ°.",
            "knowledge": f"Ø¨Ø®ØµÙˆØµ '{message}'ØŒ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ØªØ§Ø­Ø© ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø¹Ø¯Ø© Ø¬ÙˆØ§Ù†Ø¨ Ù…Ù‡Ù…Ø© ÙŠØ¬Ø¨ Ù…Ø±Ø§Ø¹Ø§ØªÙ‡Ø§.",
            "creativity": f"ÙÙƒØ±Ø© Ù…Ø¨ØªÙƒØ±Ø© Ù„Ù€ '{message}': Ø¬Ø±Ø¨ Ù†Ù‡Ø¬Ø§Ù‹ Ù…Ø®ØªÙ„ÙØ§Ù‹ - Ø§Ø¨Ø¯Ø£ Ù…Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙˆØ§Ø¹Ù…Ù„ Ø¨Ø§Ù„Ø¹ÙƒØ³.",
            "security": f"Ù…Ù† Ù†Ø§Ø­ÙŠØ© Ø§Ù„Ø£Ù…Ø§Ù†ØŒ '{message}' ÙŠØ­ØªØ§Ø¬ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø£ÙˆÙ„Ø§Ù‹. ØªØ£ÙƒØ¯ Ù…Ù† Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.",
            "performance": f"Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ '{message}': Ø­Ù„Ù„ Ù†Ù‚Ø§Ø· Ø§Ù„Ø§Ø®ØªÙ†Ø§Ù‚ Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø«Ù… Ø­Ø³Ù‘Ù† Ø§Ù„Ø£Ù‡Ù… ÙØ§Ù„Ø£Ù‡Ù….",
            "execution": f"Ù„ØªÙ†ÙÙŠØ° '{message}': Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù‡ÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨Ø¯Ù‚Ø©ØŒ Ø«Ù… Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙˆÙ„ÙŠ.",
            "learning": f"Ù„Ù„ØªØ¹Ù„Ù… Ù…Ù† '{message}': ÙƒÙ„ ØªØ¬Ø±Ø¨Ø© ÙØ±ØµØ© Ù„Ù„ØªØ·ÙˆØ±. Ø­Ù„Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ·ÙˆÙ‘Ø± Ø£Ø³Ù„ÙˆØ¨Ùƒ.",
            "decision": f"Ù‚Ø±Ø§Ø±ÙŠ Ø¨Ø®ØµÙˆØµ '{message}': Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ØŒ Ø£Ù†ØµØ­ Ø¨Ø§Ù„Ù…Ø¶ÙŠ Ù‚Ø¯Ù…Ø§Ù‹ Ù…Ø¹ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬.",
        }
        response = fallback_responses.get(role, f"ÙƒÙ€{wise_man_name}ØŒ Ø±Ø³Ø§Ù„ØªÙƒ Ù…Ù‡Ù…Ø©. Ø£Ù‚ØªØ±Ø­ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù† Ø¹Ø¯Ø© Ø²ÙˆØ§ÙŠØ§.")
    return response


def _load_training_knowledge(query: str) -> str:
    try:
        data_dirs = [
            TRAINING_DATA_DIR / "learning",
            TRAINING_DATA_DIR / "memory",
            Path("/home/bi/training_data/learning_data"),
        ]
        knowledge_items = []
        for data_dir in data_dirs:
            if data_dir.exists():
                for f in data_dir.rglob("*.jsonl"):
                    try:
                        lines = f.read_text(encoding="utf-8", errors="ignore").strip().split("\n")
                        for line in lines[-5:]:
                            try:
                                item = json.loads(line)
                                if query.lower() in json.dumps(item, ensure_ascii=False).lower():
                                    knowledge_items.append(item)
                            except:
                                pass
                    except:
                        pass
                    if len(knowledge_items) >= 3:
                        break
        if knowledge_items:
            summaries = []
            for item in knowledge_items[:3]:
                text = item.get("response", item.get("text", item.get("output", str(item)[:200])))
                summaries.append(str(text)[:200])
            return "\n".join(summaries)
    except Exception as e:
        print(f"Knowledge load error: {e}")
    return ""


# ================================================================
#  API Endpoints
# ================================================================

@app.get("/")
def root():
    gpu_info = get_gpu_info()
    return {
        "name": "ğŸš€ RTX 5090 AI & Training Server",
        "version": "4.0",
        "gpu": gpu_info.get("available", False) and torch.cuda.get_device_name(0) if TORCH_AVAILABLE and torch.cuda.is_available() else "CPU Mode",
        "pytorch_available": TORCH_AVAILABLE,
        "cuda_available": TORCH_AVAILABLE and torch.cuda.is_available() if TORCH_AVAILABLE else False,
        "resource_manager": RESOURCE_MANAGER_AVAILABLE,
        "status": "ğŸŸ¢ Online",
        "council_available": True,
        "training_data_size": sum(f.stat().st_size for f in TRAINING_DATA_DIR.rglob("*") if f.is_file()) if TRAINING_DATA_DIR.exists() else 0,
    }

@app.get("/health")
def health():
    return {"status": "ok", "version": "4.0", "timestamp": datetime.now().isoformat()}


# ==================== Resource Management API ====================

@app.get("/resources/status")
def resources_status():
    """Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø­ÙŠØ© â€” CPU, GPU, RAM, Training"""
    if not RESOURCE_MANAGER_AVAILABLE:
        # Fallback: basic system info
        gpu_info = get_gpu_info()
        return {
            "config": {"cpu_percent": 0, "gpu_percent": 0},
            "system": {
                "gpu_available": gpu_info["available"],
                "gpu_utilization": gpu_info["usage"],
                "gpu_temp_c": gpu_info["temp"],
                "gpu_vram_used_gb": gpu_info["vram_used"],
                "gpu_vram_total_gb": gpu_info["vram_total"],
            },
            "training_active": False,
            "resource_manager": False,
        }
    return res_manager.get_live_status()


@app.post("/resources/configure")
def resources_configure(config: ResourceConfigRequest):
    """Ø¶Ø¨Ø· Ù†Ø³Ø¨Ø© Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ (CPU/GPU/RAM)"""
    if not RESOURCE_MANAGER_AVAILABLE:
        raise HTTPException(status_code=503, detail="ResourceManager not available")
    
    result = res_manager.configure(
        cpu_percent=config.cpu_percent,
        gpu_percent=config.gpu_percent,
        ram_limit_percent=config.ram_limit_percent,
    )
    return result


@app.post("/resources/training/start")
def start_intensive_training(req: IntensiveTrainingRequest):
    """Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù…ÙƒØ«Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
    if not RESOURCE_MANAGER_AVAILABLE:
        raise HTTPException(status_code=503, detail="ResourceManager not available")
    
    # Configure resources first
    res_manager.configure(
        cpu_percent=req.cpu_percent,
        gpu_percent=req.gpu_percent,
    )
    
    result = res_manager.start_training(epochs=req.epochs)
    return result


@app.post("/resources/training/stop")
def stop_intensive_training():
    """Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙƒØ«Ù"""
    if not RESOURCE_MANAGER_AVAILABLE:
        raise HTTPException(status_code=503, detail="ResourceManager not available")
    return res_manager.stop_training()


@app.get("/resources/system")
def system_resources():
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© Ø¹Ù† Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…"""
    if RESOURCE_MANAGER_AVAILABLE:
        return detect_system_resources()
    return get_gpu_info()


# ==================== Legacy Training API ====================

@app.get("/status")
def get_status():
    """Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù€ GPU"""
    gpu_info = get_gpu_info()
    if RESOURCE_MANAGER_AVAILABLE and res_manager.training_active:
        m = res_manager.metrics
        return {
            "is_training": True,
            "current_epoch": m.get("epoch", 0),
            "total_epochs": m.get("total_epochs", 0),
            "loss": m.get("loss", 0),
            "accuracy": m.get("accuracy", 0),
            "gpu_usage": m.get("gpu_utilization", gpu_info["usage"]),
            "gpu_temp": m.get("gpu_temp_c", gpu_info["temp"]),
            "vram_used": m.get("gpu_vram_used_gb", gpu_info["vram_used"]),
            "vram_total": m.get("gpu_vram_total_gb", gpu_info["vram_total"]),
            "progress": round(m.get("epoch", 0) / max(1, m.get("total_epochs", 1)) * 100, 1),
            "model_preset": m.get("model_preset", ""),
            "model_params": m.get("model_params", 0),
            "throughput_samples_sec": m.get("throughput_samples_sec", 0),
            "cpu_utilization": m.get("cpu_utilization", 0),
            "cuda_available": gpu_info["available"],
            "device": "cuda" if gpu_info["available"] else "cpu",
        }
    return {
        "is_training": False,
        "gpu_usage": gpu_info["usage"],
        "gpu_temp": gpu_info["temp"],
        "vram_used": gpu_info["vram_used"],
        "vram_total": gpu_info["vram_total"],
        "cuda_available": gpu_info["available"],
        "device": "cuda" if gpu_info["available"] else "cpu",
    }


@app.post("/start")
def start_training(config: Optional[TrainingConfig] = None):
    """Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (ÙŠØ³ØªØ®Ø¯Ù… ResourceManager)"""
    if RESOURCE_MANAGER_AVAILABLE:
        cfg = config or TrainingConfig()
        return res_manager.start_training(epochs=cfg.epochs)
    return {"status": "error", "message": "ResourceManager not available"}


@app.post("/stop")
def stop_training():
    """Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    if RESOURCE_MANAGER_AVAILABLE:
        return res_manager.stop_training()
    return {"status": "not_running"}


@app.get("/gpu/info")
def gpu_info_endpoint():
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© Ø¹Ù† Ø§Ù„Ù€ GPU"""
    info = get_gpu_info()
    if TORCH_AVAILABLE and torch.cuda.is_available():
        return {
            **info,
            "name": torch.cuda.get_device_name(0),
            "cuda_version": torch.version.cuda,
            "pytorch_version": torch.__version__,
            "memory_cached": torch.cuda.memory_reserved() / 1e9,
            "device_count": torch.cuda.device_count()
        }
    return info


# ==================== Council Endpoint ====================

@app.post("/council/message")
def council_message(request: CouncilMessageRequest):
    """AI Council message â€” Ø­ÙƒÙŠÙ… ÙŠØ¬Ø§ÙˆØ¨"""
    wise_man = _select_wise_man(request.message)
    response = _generate_council_response(request.message, wise_man)

    council_chat_history.append({
        "role": "user", "message": request.message,
        "timestamp": datetime.now().isoformat(),
    })
    council_chat_history.append({
        "role": "council", "council_member": wise_man,
        "message": response, "timestamp": datetime.now().isoformat(),
    })
    while len(council_chat_history) > 100:
        council_chat_history.pop(0)

    return {
        "response": response,
        "council_member": wise_man,
        "model_used": "rtx5090-local",
        "source": "rtx5090",
    }


# ==================== Training Data Ingestion ====================

@app.post("/api/v1/training-data/ingest")
def ingest_training_data(request: TrainingIngestRequest):
    INGEST_DIR.mkdir(parents=True, exist_ok=True)
    filepath = INGEST_DIR / f"ingest_{datetime.now().strftime('%Y%m%d')}.jsonl"
    count = 0
    with open(filepath, "a", encoding="utf-8") as f:
        for sample in request.samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
            count += 1
    return {"status": "ok", "ingested": count, "file": str(filepath)}


# ==================== Main ====================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ RTX 5090 AI & Training Server v4.0")
    print("âš¡ Real Hardware Resource Utilization")
    print("=" * 60)

    if TORCH_AVAILABLE:
        print(f"ğŸ“¦ PyTorch: {torch.__version__}")
        if torch.cuda.is_available():
            print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            print(f"ğŸ”§ CUDA: {torch.version.cuda}")
        else:
            print("âš ï¸ CUDA not available - using CPU")
    else:
        print("âš ï¸ PyTorch not installed - mock mode")

    if RESOURCE_MANAGER_AVAILABLE:
        info = res_manager.system_info
        print(f"ğŸ–¥ï¸ CPU: {info.get('cpu_cores_physical', '?')} cores ({info.get('cpu_cores_logical', '?')} logical)")
        print(f"ğŸ§  RAM: {info.get('ram_total_gb', '?')} GB")
        print("âœ… ResourceManager: Active")
    else:
        print("âš ï¸ ResourceManager: Not available")

    # Training data
    if TRAINING_DATA_DIR.exists():
        total_size = sum(f.stat().st_size for f in TRAINING_DATA_DIR.rglob("*") if f.is_file())
        print(f"ğŸ“Š Training Data: {total_size / 1e9:.1f} GB")

    print("=" * 60)
    print("ğŸ“¡ API Endpoints:")
    print("  GET  /                        â†’ Server info")
    print("  GET  /health                  â†’ Health check")
    print("  GET  /resources/status         â†’ Live resource usage")
    print("  POST /resources/configure      â†’ Set CPU/GPU %")
    print("  POST /resources/training/start â†’ Start intensive training")
    print("  POST /resources/training/stop  â†’ Stop training")
    print("  GET  /resources/system         â†’ System hardware info")
    print("  POST /council/message          â†’ AI Council")
    print("=" * 60)

    uvicorn.run(app, host="0.0.0.0", port=8090)
